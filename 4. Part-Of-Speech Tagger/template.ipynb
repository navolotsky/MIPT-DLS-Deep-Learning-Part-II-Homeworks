{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[homework]language_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":false,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ot3c4fjZwC4T"},"source":["<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"P2JdzEXmwRU5"},"source":["---"]},{"cell_type":"markdown","metadata":{"id":"oMohh_6CwC4W"},"source":["### Задача определения частей речи, Part-Of-Speech Tagger (POS)"]},{"cell_type":"markdown","metadata":{"id":"2Aad2tmBwC4Y"},"source":["Мы будем решать задачу определения частей речи (POS-теггинга) с помощью скрытой марковской модели (HMM)."]},{"cell_type":"code","metadata":{"id":"gYYV0mdmwC4f","scrolled":false},"source":["import nltk\n","import pandas as pd\n","import numpy as np\n","from collections import OrderedDict, deque\n","from nltk.corpus import brown\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FPgI52lRwC4n"},"source":["Вам в помощь http://www.nltk.org/book/"]},{"cell_type":"markdown","metadata":{"id":"hxdJxMEAwC4o"},"source":["Загрузим brown корпус"]},{"cell_type":"code","metadata":{"id":"ZvhXAL_9wC4q","scrolled":true},"source":["nltk.download('brown')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ASm3Dpggs25b"},"source":["Существует множество наборов грамматических тегов, или тегсетов, например:\n","* НКРЯ\n","* Mystem\n","* UPenn\n","* OpenCorpora (его использует pymorphy2)\n","* Universal Dependencies"]},{"cell_type":"markdown","metadata":{"id":"wto8PSC6wC4v"},"source":["<b>Существует не одна система тегирования, поэтому будьте внимательны, когда прогнозируете тег слов в тексте и вычисляете качество прогноза. Можете получить несправедливо низкое качество вашего решения."]},{"cell_type":"markdown","metadata":{"id":"eJ6tuHA_wC4z"},"source":["На данный момент стандартом является **Universal Dependencies**. Подробнее про проект можно почитать [вот тут](http://universaldependencies.org/), а про теги — [вот тут](http://universaldependencies.org/u/pos/)"]},{"cell_type":"code","metadata":{"id":"Cht7dImWwC42"},"source":["nltk.download('universal_tagset')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IiTimRRywC47"},"source":["<img src=\"https://4.bp.blogspot.com/-IcFli2wljs0/WrVCw3umY_I/AAAAAAAACYM/UJ_neoUAs3wF95dj2Ouf3BzxXzB_b2TbQCLcBGAs/s1600/postags.png\">\n"]},{"cell_type":"markdown","metadata":{"id":"iyDBMcBSwC48"},"source":["Мы имеем массив предложений пар (слово-тег)"]},{"cell_type":"code","metadata":{"id":"BobflewQwC4-","scrolled":false},"source":["brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")\n","brown_tagged_sents"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jSu1KqRrwC5L"},"source":["Первое предложение"]},{"cell_type":"code","metadata":{"id":"zCHCZPlkwC5N"},"source":["brown_tagged_sents[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIV2MiRxwC5Q"},"source":["Все пары (слово-тег)"]},{"cell_type":"code","metadata":{"id":"dVx9e9HcwC5R"},"source":["brown_tagged_words = brown.tagged_words(tagset='universal')\n","brown_tagged_words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-ADby6LwC5V"},"source":["Проанализируйте данные, с которыми Вы работаете. Используйте `nltk.FreqDist()` для подсчета частоты встречаемости тега и слова в нашем корпусе. Под частой элемента подразумевается кол-во этого элемента в корпусе."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"JzRoXuKFcMZK"},"source":["# Приведем слова к нижнему регистру\n","brown_tagged_words = list(map(lambda x: (x[0].lower(), x[1]), brown_tagged_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4giWaqXjwC5W"},"source":["print('Кол-во предложений: ', len(brown_tagged_sents))\n","tags = [tag for (word, tag) in brown_tagged_words] # наши теги\n","words = [word for (word, tag) in brown_tagged_words] # наши слова\n","\n","tag_num = pd.Series('''your code''').sort_values(ascending=False) # тег - кол-во тега в корпусе\n","word_num = pd.Series('''your code''').sort_values(ascending=False) # слово - кол-во слова в корпусе"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfiPpCcLwC5Z","scrolled":true},"source":["tag_num"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Y1huw7TwC5b"},"source":["plt.figure(figsize=(12, 5))\n","plt.bar(tag_num.index, tag_num.values)\n","plt.title(\"Tag_frequency\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gBbhnJsmwC5f"},"source":["word_num[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WmEOBMkwC5i"},"source":["plt.figure(figsize=(12, 5))\n","plt.bar(word_num.index[:10], word_num.values[:10])\n","plt.title(\"Word_frequency\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n08z2PjMwC5o"},"source":["### Вопрос 1:\n","* Кол-во слова `cat` в корпусе?"]},{"cell_type":"code","metadata":{"id":"jhB7di3YwC5p"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UsCfVLsewC5s"},"source":["### Вопрос 2:\n","* Самое популярное слово с самым популярным тегом? <br>(*сначала выбираете слова с самым популярным тегом, а затем выбираете самое популярное слово из уже выбранных*)"]},{"cell_type":"code","metadata":{"id":"oio-XBYkwC5t"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K-OGc1rSwC5x"},"source":["Впоследствии обучение моделей может занимать слишком много времени, работайте с подвыборкой, например, только текстами определенных категорий."]},{"cell_type":"markdown","metadata":{"id":"Eb7MhxVRwC5y"},"source":["Категории нашего корпуса:"]},{"cell_type":"code","metadata":{"id":"GSiVcP1TwC51"},"source":["brown.categories()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MjSlFatJwC53"},"source":["Будем работать с категорией humor"]},{"cell_type":"markdown","metadata":{"id":"_f1rl5x0wC55"},"source":["Cделайте случайное разбиение выборки на обучение и контроль в отношении 9:1. "]},{"cell_type":"code","metadata":{"id":"GX9t-1qowC58"},"source":["brown_tagged_sents = brown.tagged_sents(tagset=\"universal\", categories='humor')\n","# Приведем слова к нижнему регистру\n","my_brown_tagged_sents = []\n","for sent in brown_tagged_sents:\n","    my_brown_tagged_sents.append(list(map(lambda x: (x[0].lower(), x[1]), sent)))\n","my_brown_tagged_sents = np.array(my_brown_tagged_sents)\n","\n","from sklearn.model_selection import train_test_split\n","train_sents, test_sents = train_test_split('''your code''', random_state=0,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pXkVwUjYwC5-"},"source":["len(train_sents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JQMjzJ2YwC6C"},"source":["len(test_sents)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rEasLVcwC6G"},"source":["### Метод максимального правдоподобия для обучения модели\n","\n","* $\\normalsize S = s_0, s_1, ..., s_N$ - скрытые состояния, то есть различные теги\n","* $\\normalsize O = o_0, o_1, ..., o_M$ - различные слова\n","* $\\normalsize a_{i,j} = p(s_j|s_i)$ - вероятность того, что, находясь в скрытом состоянии $s_i$, мы попадем в состояние $s_j$ (элемент матрицы $A$)\n","* $\\normalsize b_{k,j}=p(o_k|s_j)$ - вероятность того, что при скрытом состоянии $s_j$ находится слово $o_k$(элемент матрицы $B$)\n","\n","$$\\normalsize x_t \\in O, y_t \\in S$$\n","$\\normalsize (x_t, y_t)$ - слово и тег, стоящие на месте $t$ $\\Rightarrow$ \n","* $\\normalsize X$ - последовательность слов\n","* $\\normalsize Y$ - последовательность тегов\n","\n","Требуется построить скрытую марковскую модель (class HiddenMarkovModel) и написать метод fit для настройки всех её параметров с помощью оценок максимального правдоподобия по размеченным данным (последовательности пар слово+тег):\n","\n","- Вероятности переходов между скрытыми состояниями $p(y_t | y_{t - 1})$ посчитайте на основе частот биграмм POS-тегов.\n","\n","\n","- Вероятности эмиссий наблюдаемых состояний $p(x_t | y_t)$ посчитайте на основе частот \"POS-тег - слово\".\n","\n","\n","- Распределение вероятностей начальных состояний $p(y_0)$ задайте равномерным.\n","\n","Пример $X = [x_0, x_1], Y = [y_0, y_1]$:<br><br>\n","$$p(X, Y) = p(x_0, x_1, y_0, y_1) = p(y_0) \\cdot p(x_0, x_1, y_1 | y_0) = p(y_0) \\cdot p(x_0 | y_0) \\cdot\n","p(x_1, y_1 | x_0, y_0) = \\\\ = p(y_0) \\cdot p(x_0 | y_0) \\cdot p(y_1 | x_0, y_0) \\cdot p(x_1 | x_0, y_0, y_1)\n","= (\\text{в силу условий нашей модели}) = \\\\ = p(y_0) \\cdot p(x_0 | y_0) \\cdot p(y_1 | y_0) \\cdot p(x_1 | y_1) \\Rightarrow$$ <br>\n","Для последовательности длины $n + 1$:<br>\n","$$p(X, Y) = p(x_0 ... x_{n - 1}, y_0 ... y_{n - 1}) \\cdot p(y_n | y_{n - 1}) \\cdot p(x_n | y_n)$$"]},{"cell_type":"markdown","metadata":{"id":"tysPoe5rwC6I"},"source":["#### Алгоритм Витерби для применения модели\n","\n","\n","Требуется написать метод .predict для определения частей речи на тестовой выборке. Чтобы использовать обученную модель на новых данных, необходимо реализовать алгоритм Витерби. Это алгоритм динамиеского программирования, с помощью которого мы будем находить наиболее вероятную последовательность скрытых состояний модели для фиксированной последовательности слов:\n","\n","$$ \\hat{Y} = \\arg \\max_{Y} p(Y|X) = \\arg \\max_{Y} p(Y, X) $$\n","\n","Пусть $\\normalsize Q_{t,s}$ - самая вероятная последовательность скрытых состояний длины $t$ с окончанием в состоянии $s$. $\\normalsize q_{t, s}$ - вероятность этой последовательности.\n","$$(1)\\: \\normalsize q_{t,s} = \\max_{s'} q_{t - 1, s'} \\cdot p(s | s') \\cdot p(o_t | s)$$\n","$\\normalsize Q_{t,s}$ можно восстановить по argmax-ам."]},{"cell_type":"code","metadata":{"id":"QpEXdhOfwC6J"},"source":["class HiddenMarkovModel:    \n","    def __init__(self):\n","    \n","        pass\n","        \n","    def fit(self, train_tokens_tags_list):\n","        \"\"\"\n","        train_tokens_tags_list: массив предложений пар слово-тег (выборка для train) \n","        \"\"\"\n","        tags = [tag for sent in train_tokens_tags_list\n","                for (word, tag) in sent]\n","        words = [word for sent in train_tokens_tags_list\n","                 for (word, tag) in sent]\n","        \n","        tag_num = pd.Series('''your code''').sort_index()\n","        word_num = pd.Series('''your code''').sort_values(ascending=False)\n","         \n","        self.tags = tag_num.index\n","        self.words = word_num.index\n","        \n","        A = pd.DataFrame({'{}'.format(tag) : [0] * len(tag_num) for tag in tag_num.index}, index=tag_num.index)\n","        B = pd.DataFrame({'{}'.format(tag) : [0] * len(word_num) for tag in tag_num.index}, index=word_num.index)\n","        \n","        # Вычисляем матрицу A и B по частотам слов и тегов\n","        \n","        # sent - предложение\n","        # sent[i][0] - i слово в этом предложении, sent[i][1] - i тег в этом предложении\n","        for sent in train_tokens_tags_list:\n","            for i in range(len(sent)):\n","                B.loc['''your code'''] += 1 # текущая i-пара слово-тег (обновите матрицу B аналогично A)\n","                if len(sent) - 1 != i: # для последнего тега нет следующего тега\n","                    A.loc[sent[i][1], sent[i + 1][1]] += 1 # пара тег-тег\n","                \n","        \n","        # переходим к вероятностям\n","        \n","        # нормируем по строке, то есть по всем всевозможным следующим тегам\n","        A = A.divide(A.sum(axis=1), axis=0)\n","        \n","        # нормируем по столбцу, то есть по всем всевозможным текущим словам\n","        B = B / np.sum(B, axis=0)\n","        \n","        self.A = A\n","        self.B = B\n","        \n","        return self\n","        \n","    \n","    def predict(self, test_tokens_list):\n","        \"\"\"\n","        test_tokens_list : массив предложений пар слово-тег (выборка для test)\n","        \"\"\"\n","        predict_tags = OrderedDict({i : np.array([]) for i in range(len(test_tokens_list))})\n","        \n","        for i_sent in range(len(test_tokens_list)):\n","            \n","            current_sent = test_tokens_list[i_sent] # текущее предложение\n","            len_sent = len(current_sent) # длина предложения \n","            \n","            q = np.zeros(shape=(len_sent + 1, len(self.tags)))\n","            q[0] = 1 # нулевое состояние (равномерная инициализация по всем s)\n","            back_point = np.zeros(shape=(len_sent + 1, len(self.tags))) # # argmax\n","            \n","            for t in range(len_sent):\n","                \n","                # если мы не встречали такое слово в обучении, то вместо него будет \n","                # самое популярное слово с самым популярным тегом (вопрос 2)\n","                if current_sent[t] not in self.words:\n","                    current_sent[t] = '''your code'''\n","                    \n","                # через max выбираем следующий тег\n","                for i_s in range(len(self.tags)):\n","                    \n","                    s = self.tags[i_s]\n","                    \n","                    # формула (1)\n","                    q[t + 1][i_s] = np.max(q['''your code'''] *\n","                        self.A.loc[:, '''your code'''] * \n","                        self.B.loc[current_sent[t], s])\n","                    \n","                    # argmax формула(1)\n","                    \n","                    # argmax, чтобы восстановить последовательность тегов\n","                    back_point[t + 1][i_s] = (q['''your code'''] * self.A.loc[:, '''your code'''] * \n","                        self.B.loc[current_sent[t],s]).reset_index()[s].idxmax() # индекс \n","                    \n","            back_point = back_point.astype('int')\n","            \n","            # выписываем теги, меняя порядок на реальный\n","            back_tag = deque()\n","            current_tag = np.argmax(q[len_sent])\n","            for t in range(len_sent, 0, -1):\n","                back_tag.appendleft(self.tags[current_tag])\n","                current_tag = back_point[t, current_tag]\n","             \n","            predict_tags[i_sent] = np.array(back_tag)\n","        \n","        \n","        return predict_tags                 "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y0BLgsWkwC6M"},"source":["Обучите скрытую марковскую модель:"]},{"cell_type":"code","metadata":{"id":"ZcSoyUAxwC6M"},"source":["# my_model = ..,\n","'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FeVNt19kwC6P"},"source":["Проверьте работу реализованного алгоритма на следующих модельных примерах, проинтерпретируйте результат.\n","\n","- 'He can stay'\n","- 'a cat and a dog'\n","- 'I have a television'\n","- 'My favourite character'"]},{"cell_type":"code","metadata":{"id":"cMJErf7NwC6Q"},"source":["sents = [['He', 'can', 'stay'], ['a', 'cat', 'and', 'a', 'dog'], ['I', 'have', 'a', 'television'],\n","         ['My', 'favourite', 'character']]\n","'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"suDCwbGMwC6T"},"source":["### Вопрос 3:\n","* Какой тег вы получили для слова `can`?"]},{"cell_type":"code","metadata":{"id":"ReHeG3IjwC6U"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ObAslurlwC6X"},"source":["### Вопрос 4:\n","* Какой тег вы получили для слова `favourite`?"]},{"cell_type":"code","metadata":{"id":"94crVrrXwC6Y"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YPC4NZ4HwC6a"},"source":["Примените модель к отложенной выборке Брауновского корпуса и подсчитайте точность определения тегов (accuracy). Сделайте выводы. "]},{"cell_type":"code","metadata":{"id":"-7aioBc1wC6b"},"source":["def accuracy_score(model, sents):\n","    true_pred = 0\n","    num_pred = 0\n","\n","    for sent in sents:\n","        tags = '''your code'''\n","        words = '''your code'''\n","\n","        '''your code'''\n","\n","        true_pred += '''your code'''\n","        num_pred += '''your code'''\n","    print(\"Accuracy:\", true_pred / num_pred * 100, '%')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"roesKrPCcMbp"},"source":["accuracy_score(my_model, test_sents)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ff_W7J8XwC6e"},"source":["### Вопрос 5:\n","* Какое качество вы получили(округлите до одного знака после запятой)?"]},{"cell_type":"code","metadata":{"id":"ptvlpc-6wC6f"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FpAgfZRTwC6h"},"source":["## DefaultTagger"]},{"cell_type":"markdown","metadata":{"id":"9b4cPKyiwC6j"},"source":["### Вопрос 6:\n","* Какое качество вы бы получили, если бы предсказывали любой тег, как самый популярный тег на выборке train(округлите до одного знака после запятой)?"]},{"cell_type":"markdown","metadata":{"id":"Td-0Pe0vwC6k"},"source":["Вы можете испоьзовать DefaultTagger(метод tag для предсказания частей речи предложения)"]},{"cell_type":"code","metadata":{"id":"NfZYlMxJwC6m"},"source":["from nltk.tag import DefaultTagger\n","default_tagger = DefaultTagger('''your code''')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CXKibo_cMcB"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lz7Q3BfbwC6o"},"source":["## NLTK, Rnnmorph"]},{"cell_type":"markdown","metadata":{"id":"PZvdB22Oyycz"},"source":["Вспомним первый [семинар](https://colab.research.google.com/drive/1FHZVU6yJT61J8w1hALno0stD4VU36rit?usp=sharing) нашего курса. В том семинаре мы с вами работали c некоторыми библиотеками.\n","\n","Не забудьте преобразовать систему тэгов из `'en-ptb' в 'universal'` с помощью функции `map_tag` или используйте `tagset='universal'`"]},{"cell_type":"code","metadata":{"id":"9bn1TGlGAfuL"},"source":["from nltk.tag.mapping import map_tag"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JJQFfbp8A_cj"},"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","# nltk.pos_tag(..., tagset='universal')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8LD_61W7N35q"},"source":["from rnnmorph.predictor import RNNMorphPredictor\n","predictor = RNNMorphPredictor(language=\"en\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i1z8x4vvwC6s"},"source":["### Вопрос 7:\n","* Какое качество вы получили, используя каждую из двух библиотек? Сравните их результаты.\n","\n","* Качество с библиотекой rnnmorph должно быть хуже, так как там используется немного другая система тэгов. Какие здесь отличия?"]},{"cell_type":"code","metadata":{"id":"GBd3RgqVwC6s"},"source":["'''your code'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5w1W5hSkcMcV"},"source":["## BiLSTMTagger"]},{"cell_type":"markdown","metadata":{"id":"mm1-S3t2cMcW"},"source":["### Подготовка данных"]},{"cell_type":"markdown","metadata":{"id":"GayTl7mUcMcX"},"source":["Изменим структуру данных"]},{"cell_type":"code","metadata":{"id":"CnXcI64fxoj4","scrolled":false},"source":["pos_data = [list(zip(*sent)) for sent in brown_tagged_sents]\n","print(pos_data[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpRE3c-3cMcc"},"source":["До этого мы писали много кода сами, теперь пора эксплуатировать pytorch"]},{"cell_type":"code","metadata":{"id":"gvFlzrYnxokE"},"source":["from torchtext.legacy.data import Field, BucketIterator\n","import torchtext\n","\n","# наши поля\n","WORD = Field(lower=True)\n","TAG = Field(unk_token=None) # все токены нам извсетны\n","\n","# создаем примеры\n","examples = []\n","for words, tags in pos_data:\n","    examples.append(torchtext.legacy.data.Example.fromlist([list(words), list(tags)], fields=[('words', WORD), ('tags', TAG)]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tjl6u6cpOc1u"},"source":["Вот один наш пример:"]},{"cell_type":"code","metadata":{"id":"dnrzktytN9rL"},"source":["print(vars(examples[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUhTrWCWcMcj"},"source":["Теперь формируем наш датасет"]},{"cell_type":"code","metadata":{"id":"LGKkbZUIxokO","scrolled":true},"source":["# кладем примеры в наш датасет\n","dataset = torchtext.legacy.data.Dataset('''your code''', fields=[('words', WORD), ('tags', TAG)])\n","\n","train_data, valid_data, test_data = dataset.split(split_ratio=[0.8, 0.1, 0.1])\n","\n","print(f\"Number of training examples: {len(train_data.examples)}\")\n","print(f\"Number of validation examples: {len(valid_data.examples)}\")\n","print(f\"Number of testing examples: {len(test_data.examples)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T89unpppcMcp"},"source":["Построим словари. Параметр `min_freq` выберете сами. При построении словаря испольузем только **train**"]},{"cell_type":"code","metadata":{"id":"tZwkwhlrxoka","scrolled":true},"source":["WORD.build_vocab('''your code''', min_freq='''your code''')\n","TAG.build_vocab('''your code''')\n","\n","print(f\"Unique tokens in source (ru) vocabulary: {len(WORD.vocab)}\")\n","print(f\"Unique tokens in target (en) vocabulary: {len(TAG.vocab)}\")\n","\n","print(WORD.vocab.itos[::200])\n","print(TAG.vocab.itos)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjn07NP-xokl","scrolled":true},"source":["print(vars(train_data.examples[9]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LxgkU4cZcMcz"},"source":["Посмотрим с насколько большими предложениями мы имеем дело"]},{"cell_type":"code","metadata":{"id":"dVpMi1_0xoku","scrolled":true},"source":["length = map(len, [vars(x)['words'] for x in train_data.examples])\n","\n","plt.figure(figsize=[8, 4])\n","plt.title(\"Length distribution in Train data\")\n","plt.hist(list(length), bins=20);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yi28N2RBcMc5"},"source":["Для обучения `BiLSTM` лучше использовать colab"]},{"cell_type":"code","metadata":{"id":"LAGSrqWsxok2","scrolled":true},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2DSWm0UjcMc-"},"source":["Для более быстрого и устойчивого обучения сгруппируем наши данные по батчам"]},{"cell_type":"code","metadata":{"id":"dmwAyhNgxok_"},"source":["# бьем нашу выборку на батч, не забывая сначала отсортировать выборку по длине\n","def _len_sort_key(x):\n","    return len(x.words)\n","\n","BATCH_SIZE = 32\n","\n","train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n","    (train_data, valid_data, test_data), \n","    batch_size = BATCH_SIZE, \n","    device = device,\n","    sort_key=_len_sort_key\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6aTjW00nxolI"},"source":["# посморим  на количество батчей\n","list(map(len, [train_iterator, valid_iterator, test_iterator]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zyLQsizhcMdI"},"source":["### Модель и её обучение"]},{"cell_type":"markdown","metadata":{"id":"-i9oHzcrcMdJ"},"source":["Инициализируем нашу модель"]},{"cell_type":"code","metadata":{"id":"Ff7BLWs_xolS","scrolled":true},"source":["class LSTMTagger(nn.Module):\n","\n","    def __init__(self, input_dim, emb_dim, hid_dim, output_dim, dropout, bidirectional=False):\n","        super().__init__()\n","        \n","  \n","        self.embeddings = '''your code'''\n","        self.dropout = '''your code'''\n","        \n","        self.rnn = '''your code'''\n","        # если bidirectional, то предсказываем на основе конкатенации двух hidden\n","        self.tag = nn.Linear((1 + bidirectional) * hid_dim, output_dim)\n","\n","    def forward(self, sent):\n","        \n","        #sent = [sent len, batch size] \n","        \n","        # не забываем применить dropout к embedding\n","        embedded = '''your code'''\n","\n","        output, _ = '''your code'''\n","        #output = [sent len, batch size, hid dim * n directions]\n","\n","        prediction = '''your code'''\n","    \n","        return prediction\n","        \n","# параметры модели\n","INPUT_DIM = '''your code'''\n","OUTPUT_DIM = '''your code'''\n","EMB_DIM = '''your code'''\n","HID_DIM = '''your code'''\n","DROPOUT = '''your code'''\n","BIDIRECTIONAL = '''your code'''\n","\n","model = LSTMTagger('''your code''').to(device)\n","\n","# инициализируем веса\n","def init_weights(m):\n","    for name, param in m.named_parameters():\n","        nn.init.uniform_(param, -0.08, 0.08)\n","        \n","model.apply(init_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJLqq8IHcMdQ"},"source":["Подсчитаем количество обучаемых параметров нашей модели"]},{"cell_type":"code","metadata":{"id":"_Auu53Kdxolm"},"source":["def count_parameters(model):\n","    return '''your code'''\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oSBfvf9HcMd9"},"source":["Погнали обучать"]},{"cell_type":"code","metadata":{"id":"AjD1Y7Rmxolu","scrolled":true},"source":["PAD_IDX = TAG.vocab.stoi['<pad>']\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n","\n","def train(model, iterator, optimizer, criterion, clip, train_history=None, valid_history=None):\n","    model.train()\n","    \n","    epoch_loss = 0\n","    history = []\n","    for i, batch in enumerate(iterator):\n","        \n","       '''your code'''\n","        \n","        optimizer.zero_grad()\n","        \n","        output = model('''your code''')\n","        \n","        #tags = [sent len, batch size]\n","        #output = [sent len, batch size, output dim]\n","        \n","        output = '''your code'''\n","        tags = tags.view(-1)\n","        \n","        #tags = [sent len * batch size]\n","        #output = [sent len * batch size, output dim]\n","        \n","        loss = criterion('''your code''')\n","        \n","        loss.backward()\n","        \n","        # Gradient clipping(решение проблемы взрыва граденты), clip - максимальная норма вектора\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n","        \n","        optimizer.step()\n","        \n","        epoch_loss += loss.item()\n","        \n","        history.append(loss.cpu().data.numpy())\n","        if (i+1)%10==0:\n","            fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 8))\n","\n","            clear_output(True)\n","            ax[0].plot(history, label='train loss')\n","            ax[0].set_xlabel('Batch')\n","            ax[0].set_title('Train loss')\n","            \n","            if train_history is not None:\n","                ax[1].plot(train_history, label='general train history')\n","                ax[1].set_xlabel('Epoch')\n","            if valid_history is not None:\n","                ax[1].plot(valid_history, label='general valid history')\n","            plt.legend()\n","            \n","            plt.show()\n","\n","        \n","    return epoch_loss / len(iterator)\n","\n","def evaluate(model, iterator, criterion):\n","    model.eval()\n","    \n","    epoch_loss = 0\n","    \n","    history = []\n","    \n","    with torch.no_grad():\n","    \n","        for i, batch in enumerate(iterator):\n","\n","            '''your code'''\n","\n","            output = model('''your code''')\n","\n","            #tags = [sent len, batch size]\n","            #output = [sent len, batch size, output dim]\n","\n","            output = '''your code'''\n","            tags = tags.view(-1)\n","\n","            #tags = [sent len * batch size]\n","            #output = [sent len * batch size, output dim]\n","\n","            loss = criterion('''your code''')\n","            \n","            epoch_loss += loss.item()\n","        \n","    return epoch_loss / len(iterator)\n","\n","def epoch_time(start_time, end_time):\n","    elapsed_time = end_time - start_time\n","    elapsed_mins = int(elapsed_time / 60)\n","    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n","    return elapsed_mins, elapsed_secs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TJdXIyTHxol2","scrolled":false},"source":["import time\n","import math\n","import matplotlib\n","matplotlib.rcParams.update({'figure.figsize': (16, 12), 'font.size': 14})\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from IPython.display import clear_output\n","\n","train_history = []\n","valid_history = []\n","\n","N_EPOCHS = '''your code'''\n","CLIP = '''your code'''\n","\n","best_valid_loss = float('inf')\n","\n","for epoch in range(N_EPOCHS):\n","    \n","    start_time = time.time()\n","    \n","    train_loss = train(model, train_iterator, optimizer, criterion, CLIP, train_history, valid_history)\n","    valid_loss = evaluate(model, valid_iterator, criterion)\n","    \n","    end_time = time.time()\n","    \n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","    \n","    if valid_loss < best_valid_loss:\n","        best_valid_loss = valid_loss\n","        torch.save(model.state_dict(), 'best-val-model.pt')\n","\n","    train_history.append(train_loss)\n","    valid_history.append(valid_loss)\n","    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fr860UPacMeI"},"source":["### Применение модели"]},{"cell_type":"code","metadata":{"id":"5sDAfAq9xol9"},"source":["def accuracy_model(model, iterator):\n","    model.eval()\n","    \n","    true_pred = 0\n","    num_pred = 0\n","    \n","    with torch.no_grad():\n","        for i, batch in enumerate(iterator):\n","\n","           '''your code'''\n","\n","            output = model('''your code''')\n","            \n","            #output = [sent len, batch size, output dim]\n","            output = '''your code'''\n","            \n","            #output = [sent len, batch size]\n","            predict_tags = output.cpu().numpy()\n","            true_tags = tags.cpu().numpy()\n","\n","            true_pred += np.sum((true_tags == predict_tags) & (true_tags != PAD_IDX))\n","            num_pred += np.prod(true_tags.shape) - (true_tags == PAD_IDX).sum()\n","        \n","    return round(true_pred / num_pred * 100, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2n0H85mxomE","scrolled":true},"source":["print(\"Accuracy:\", accuracy_model(model, test_iterator), '%')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FacTKSPJcMeP"},"source":["Вы можете улучшить качество, изменяя параметры модели. Но чтобы добиться нужного качества, вам неообходимо взять все выборку, а не только категорию `humor`."]},{"cell_type":"code","metadata":{"id":"QXqXg0gbcMeR"},"source":["#brown_tagged_sents = brown.tagged_sents(tagset=\"universal\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gnpi2R6rcMeU"},"source":["Вам неоходимо добиться качества не меньше, чем `accuracy = 93 %` "]},{"cell_type":"code","metadata":{"id":"TqD1lZuwxomK","scrolled":true},"source":["best_model = LSTMTagger(INPUT_DIM, EMB_DIM, HID_DIM, OUTPUT_DIM, DROPOUT, BIDIRECTIONAL).to(device)\n","best_model.load_state_dict(torch.load('best-val-model.pt'))\n","assert accuracy_model(best_model, test_iterator) >= 93"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nVfdJM-lcMeZ"},"source":["Пример решение нашей задачи:"]},{"cell_type":"code","metadata":{"id":"W3GUbwldxomW"},"source":["def print_tags(model, data):\n","    model.eval()\n","    \n","    with torch.no_grad():\n","        words, _ = data\n","        example = torch.LongTensor([WORD.vocab.stoi[elem] for elem in words]).unsqueeze(1).to(device)\n","        \n","        output = model(example).argmax(dim=-1).cpu().numpy()\n","        tags = [TAG.vocab.itos[int(elem)] for elem in output]\n","\n","        for token, tag in zip(words, tags):\n","            print(f'{token:15s}{tag}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"4mQoHc_EcMed"},"source":["print_tags(model, pos_data[-1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"zMIJDOBmwC6v"},"source":["## Сравните результаты моделей HiddenMarkov, LstmTagger:\n","* при обучение на маленькой части корпуса, например, на категории humor\n","* при обучении на всем корпусе"]},{"cell_type":"code","metadata":{"id":"uDdsG2AjO-sp"},"source":[""],"execution_count":null,"outputs":[]}]}