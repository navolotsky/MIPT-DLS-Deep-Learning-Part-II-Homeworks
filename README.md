Репозиторий содержит прорешанные домашние задания по второй части курса по **Deep Learning** от [Школы глубокого обучения](https://www.dlschool.org/) МФТИ.
Работы выполнены мной при прохождении весеннего запуска (2021) [этой части](https://www.dlschool.org/advanced-track) курса.
Работы сдавались через [Stepik](https://stepik.org/course/92488) и проверялись преподавателями курса.

Каждая папка представляет собой отдельное домашнее задание. Папки пронумерованы в порядке выдачи заданий.
В папках содержится:
* *task* — файл с описанием работы со Stepik
* *template* — папка/файл домашней работы в том виде, как она была задана
* *solution* — папка/файл прорешанной работы в том виде, как она была залита на Stepik


## Список работ:

1. **Simple embeddings**
    
    **Проделанная работа:** реализованы функции расчёта метрик HITS и DCG. Реализована функция ранжирования текстов на основе векторного представления слов. Обучена модель Word2Vec из gensim. Проведено сравнение качества ранжирования для различных способов пред- и постобработки, токенизаторов и эмбеддингов.
    
    **Оценка преподавателя:** 10/10


2. **Embeddings**
    
    Реализованы простейшая нейронка, цикл обучения, класс датасета и т.п. на PyTorch для задачи классификации текстов на основе предобученных эмбеддингов слов. Попробованы разные способы замены слов, отсутствующих в этих эмбеддингах.  
    
    10/10


3. **Text Classification**

    Код из *template* переведён на новую версию torchtext. Реализованы рекуррентная и сверточная нейронные сети и циклы обучения на PyTorch для задачи классификации текстов. RNN и CNN обучены совместно с эмбеддингами. CNN обучена с предобученными GloVe-эмбеддингами. Работа сетей визуализирована.

    10/10


4. **Part-Of-Speech Tagger**

    Реализованы и обучены Скрытая марковская модель и BiLSTM для задачи POS tagging. Попробованы несколько библиотечных теггеров. Проведено сравнение работы моделей.

    10/10


5. **Seq2Seq with Attention**

    Код из *template* переведён на новую версию torchtext. Реализована модель Seq2Seq (Attention, Encoder, Decoder). Обучена на 0/10.

    не проверялась


6. **Transformers+**

    Обучена GPT2 из HuggingFace для классификации текстов в двух вариантах: с нуля и Fine tuning. Проведено исследование и сравнение карт вниманий. 
    
    Для задачи sentiment analysis на датасете SST2:
    1. обучена логистическая регрессия, в качестве признаков для которой использован выход из DistilBert из HuggingFace
    2. обучена (с Fine tuning) DistilBert из HuggingFace 

    15/15


7. **Summarization**

    Реализована и обучена RNN (почти SummaRuNNer) для экстрактивной суммаризации текста. Проверена гипотеза об обучении с предобученными эмбеддингами (добавлено в конец файла *..._extra_research_appended_after_submit*).

    9/10


8. **Введение в обработку звука**

    не выполнялась
