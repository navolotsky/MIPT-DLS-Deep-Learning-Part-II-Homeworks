{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"[homework]summarization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tWJTCBu4Tveb"},"source":["<img src=\"https://static.wixstatic.com/media/66c28f_db7a1ba3e35b4b17a6688472c889b7bf~mv2_d_2777_1254_s_2.png/v1/fill/w_710,h_320,al_c,q_85,usm_0.66_1.00_0.01/logo_yellow_white.webp\" width=1000, height=450>\n","<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"]},{"cell_type":"markdown","metadata":{"id":"Kmb8UhIzOnfK"},"source":["# Text Summarization\n","\n","Всем привет! Сегодня мы познакомимся с задачей суммаризации текста на примере генерации \"сжатых\" новостей. Рассмотрим некоторые базовые решения и познакомимся с архитектурами нейросетей для решения задачи.\n","Датасет: gazeta.ru\n","\n","\n","`Ноутбук создан на основе семинара Гусева Ильи на кафедре компьютерной лингвистики ABBYY МФТИ.`\n","\n","Загрузим датасет и необходимые библиотеки"]},{"cell_type":"code","metadata":{"id":"OqkLTkFRfXvA"},"source":["!wget -q https://www.dropbox.com/s/43l702z5a5i2w8j/gazeta_train.txt\n","!wget -q https://www.dropbox.com/s/k2egt3sug0hb185/gazeta_val.txt\n","!wget -q https://www.dropbox.com/s/3gki5n5djs9w0v6/gazeta_test.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SXS1sdYZCluU"},"source":["!pip -q install razdel networkx pymorphy2 nltk rouge==0.3.1 summa "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wa0NfryxbPUP"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"eesnclfDDV3F"},"source":["Посмотрим на то, как устроен датасет"]},{"cell_type":"code","metadata":{"id":"Mz6CZYKQhnd-"},"source":["!head -n 1 gazeta_train.txt\n","!cat gazeta_train.txt | wc -l\n","!cat gazeta_val.txt | wc -l\n","!cat gazeta_test.txt | wc -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5pZ2UGS2DGjH"},"source":["import json\n","import random\n","\n","def read_gazeta_records(file_name, shuffle=True, sort_by_date=False):\n","    assert shuffle != sort_by_date\n","    records = []\n","    with open(file_name, \"r\") as r:\n","        for line in r:\n","            records.append(json.loads(line))\n","    if sort_by_date:\n","        records.sort(key=lambda x: x[\"date\"])\n","    if shuffle:\n","        random.shuffle\n","    return records"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNDp-BunEA91"},"source":["train_records = read_gazeta_records(\"gazeta_train.txt\")\n","val_records = read_gazeta_records(\"gazeta_val.txt\")\n","test_records = read_gazeta_records(\"gazeta_test.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"397gjsNfFBZ_"},"source":["from nltk.translate.bleu_score import corpus_bleu\n","from rouge import Rouge\n","\n","def calc_scores(references, predictions, metric=\"all\"):\n","    print(\"Count:\", len(predictions))\n","    print(\"Ref:\", references[-1])\n","    print(\"Hyp:\", predictions[-1])\n","\n","    if metric in (\"bleu\", \"all\"):\n","        print(\"BLEU: \", corpus_bleu([[r] for r in references], predictions))\n","    if metric in (\"rouge\", \"all\"):\n","        rouge = Rouge()\n","        scores = rouge.get_scores(predictions, references, avg=True)\n","        print(\"ROUGE: \", scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aaJKNsUGFBaA"},"source":["## Extractive RNN"]},{"cell_type":"markdown","metadata":{"id":"l3izlm8HFBaC"},"source":["### BPE\n","Для начала сделаем BPE токенизацию"]},{"cell_type":"code","metadata":{"id":"2DMVtloWFBaC"},"source":["!pip install youtokentome"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yg9T6q0wFBaF"},"source":["import youtokentome as yttm\n","\n","def train_bpe(records, model_path, model_type=\"bpe\", vocab_size=10000, lower=True):\n","    temp_file_name = \"temp.txt\"\n","    with open(temp_file_name, \"w\") as temp:\n","        for record in records:\n","            text, summary = record['text'], record['summary']\n","            if lower:\n","                summary = summary.lower()\n","                text = text.lower()\n","            if not text or not summary:\n","                continue\n","            temp.write(text + \"\\n\")\n","            temp.write(summary + \"\\n\")\n","    yttm.BPE.train(data=temp_file_name, vocab_size=vocab_size, model=model_path)\n","\n","train_bpe(train_records, \"BPE_model.bin\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jJFAJHTtFBaF"},"source":["### Словарь\n","Составим словарь для индексации токенов"]},{"cell_type":"code","metadata":{"id":"MueXtatmFBaG"},"source":["bpe_processor = yttm.BPE('BPE_model.bin')\n","vocabulary = bpe_processor.vocab()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z_C_p7tHFBaH"},"source":["### Кэш oracle summary\n","Закэшируем oracle summary, чтобы не пересчитывать их каждый раз"]},{"cell_type":"code","metadata":{"id":"Fp23tuPbFBaH","executionInfo":{"status":"ok","timestamp":1619439358923,"user_tz":-180,"elapsed":851,"user":{"displayName":"Deep Learning School","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNf0RkP5WvkU5MixKfC1Sv3mb-9QWgAbC6VcfQvA=s64","userId":"16549096980415837553"}}},"source":["from rouge import Rouge\n","import razdel\n","from tqdm.notebook import tqdm\n","\n","import copy\n","\n","def build_oracle_summary_greedy(text, gold_summary, calc_score, lower=True, max_sentences=30):\n","    '''\n","    Жадное построение oracle summary\n","    '''\n","    gold_summary = gold_summary.lower() if lower else gold_summary\n","    # Делим текст на предложения\n","    sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n","    n_sentences = len(sentences)\n","    oracle_summary_sentences = set()\n","    \n","    score = -1.0\n","    summaries = []\n","    for _ in range(n_sentences):\n","        for i in range(n_sentences):\n","            if i in oracle_summary_sentences:\n","                continue\n","            current_summary_sentences = copy.copy(oracle_summary_sentences)\n","            # Добавляем какое-то предложения к уже существующему summary\n","            current_summary_sentences.add(i)\n","            current_summary = \" \".join([sentences[index] for index in sorted(list(current_summary_sentences))])\n","            # Считаем метрики\n","            current_score = calc_score(current_summary, gold_summary)\n","            summaries.append((current_score, current_summary_sentences))\n","        # Если получилось улучшить метрики с добавлением какого-либо предложения, то пробуем добавить ещё\n","        # Иначе на этом заканчиваем\n","        best_summary_score, best_summary_sentences = max(summaries)\n","        if best_summary_score <= score:\n","            break\n","        oracle_summary_sentences = best_summary_sentences\n","        score = best_summary_score\n","    oracle_summary = \" \".join([sentences[index] for index in sorted(list(oracle_summary_sentences))])\n","    return oracle_summary, oracle_summary_sentences\n","\n","def calc_single_score(pred_summary, gold_summary, rouge):\n","    return rouge.get_scores([pred_summary], [gold_summary], avg=True)['rouge-2']['f']\n","\n","def add_oracle_summary_to_records(records, max_sentences=30, lower=True, nrows=1000):\n","    rouge = Rouge()\n","    for i, record in tqdm(enumerate(records)):\n","        if i >= nrows:\n","            break\n","        text = record[\"text\"]\n","        summary = record[\"summary\"]\n","\n","        summary = summary.lower() if lower else summary\n","        sentences = [sentence.text.lower() if lower else sentence.text for sentence in razdel.sentenize(text)][:max_sentences]\n","        oracle_summary, sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=lambda x, y: calc_single_score(x, y, rouge),\n","                                                                         lower=lower, max_sentences=max_sentences)\n","        record[\"sentences\"] = sentences\n","        record[\"oracle_sentences\"] = list(sentences_indicies)\n","        record[\"oracle_summary\"] = oracle_summary\n","\n","    return records[:nrows]\n","\n","ext_train_records = add_oracle_summary_to_records(train_records, nrows=2048)\n","ext_val_records = add_oracle_summary_to_records(val_records, nrows=256)\n","ext_test_records = add_oracle_summary_to_records(test_records, nrows=256)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UlXXc8qUHC5m"},"source":["### Составление батчей"]},{"cell_type":"code","metadata":{"id":"YATQKCuqHPo3"},"source":["import torch\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MNyxstTChK3C"},"source":["import random\n","import math\n","import razdel\n","import torch\n","import numpy as np\n","from rouge import Rouge\n","\n","\n","class BatchIterator():\n","    def __init__(self, records, vocabulary, batch_size, bpe_processor, shuffle=True, lower=True, max_sentences=30, max_sentence_length=50, device=torch.device('cpu')):\n","        self.records = records\n","        self.num_samples = len(records)\n","        self.batch_size = batch_size\n","        self.bpe_processor = bpe_processor\n","        self.shuffle = shuffle\n","        self.batches_count = int(math.ceil(self.num_samples / batch_size))\n","        self.lower = lower\n","        self.rouge = Rouge()\n","        self.vocabulary = vocabulary\n","        self.max_sentences = max_sentences\n","        self.max_sentence_length = max_sentence_length\n","        self.device = device\n","        \n","    def __len__(self):\n","        return self.batches_count\n","    \n","    def __iter__(self):\n","        indices = np.arange(self.num_samples)\n","        if self.shuffle:\n","            np.random.shuffle(indices)\n","\n","        for start in range(0, self.num_samples, self.batch_size):\n","            end = min(start + self.batch_size, self.num_samples)\n","            batch_indices = indices[start:end]\n","\n","            batch_inputs = []\n","            batch_outputs = []\n","            max_sentence_length = 0\n","            max_sentences = 0\n","            batch_records = []\n","\n","            for data_ind in batch_indices:\n","                \n","                record = self.records[data_ind]\n","                batch_records.append(record)\n","                text = record[\"text\"]\n","                summary = record[\"summary\"]\n","                summary = summary.lower() if self.lower else summary\n","\n","                if \"sentences\" not in record:\n","                    sentences = [sentence.text.lower() if self.lower else sentence.text for sentence in razdel.sentenize(text)][:self.max_sentences]\n","                else:\n","                    sentences = record[\"sentences\"]\n","                max_sentences = max(len(sentences), max_sentences)\n","                \n","                # номера предложений, которые в нашем саммари\n","                if \"oracle_sentences\" not in record:\n","                    calc_score = lambda x, y: calc_single_score(x, y, self.rouge)\n","                    sentences_indicies = build_oracle_summary_greedy(text, summary, calc_score=calc_score, lower=self.lower, max_sentences=self.max_sentences)[1]\n","                else:   \n","                    sentences_indicies = record[\"oracle_sentences\"]\n","                \n","                # inputs - индексы слов в предложении\n","                inputs = [bpe_processor.encode(sentence)[:self.max_sentence_length] for sentence in sentences]\n","                max_sentence_length = max(max_sentence_length, max([len(tokens) for tokens in inputs]))\n","                \n","                # получение метки класса предложения\n","                outputs = [int(i in sentences_indicies) for i in range(len(sentences))]\n","                batch_inputs.append(inputs)\n","                batch_outputs.append(outputs)\n","\n","            tensor_inputs = torch.zeros((self.batch_size, max_sentences, max_sentence_length), dtype=torch.long, device=self.device)\n","            # we add index 2 for padding\n","            # YOUR CODE\n","            tensor_outputs = torch.zeros((self.batch_size, max_sentences), dtype=torch.float32, device=self.device)\n","\n","            for i, inputs in enumerate(batch_inputs):\n","                for j, sentence_tokens in enumerate(inputs):\n","                    tensor_inputs[i][j][:len(sentence_tokens)] = torch.tensor(sentence_tokens, dtype=torch.int64)\n","\n","            for i, outputs in enumerate(batch_outputs):\n","                tensor_outputs[i][:len(outputs)] = torch.LongTensor(outputs)\n","\n","            tensor_outputs = tensor_outputs.long()\n","            yield {\n","                'inputs': tensor_inputs,\n","                'outputs': tensor_outputs,\n","                'records': batch_records\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ug9MIObdi03"},"source":["train_iterator = BatchIterator(ext_train_records, vocabulary, 32, bpe_processor, device=device)\n","val_iterator = BatchIterator(ext_val_records, vocabulary, 32, bpe_processor, device=device)\n","test_iterator = BatchIterator(ext_test_records, vocabulary, 32, bpe_processor, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yPlJMg0_dQM-"},"source":["## Extractor -  SummaRuNNer\n"," https://arxiv.org/pdf/1611.04230.pdf\n"]},{"cell_type":"markdown","metadata":{"id":"4BSsnfe4t1uK"},"source":["### Homework\n","\n","* В данной реализации в `outputs` в качестве padding используется индекс 0. Измените в функции \\_\\_iter__ индекс padding, чтобы он не совпадал с классом 0 или 1, например, 2.\n","* В качестве criterion используйте `CrossEntropyLoss`вместо `BCEWithLogitsLoss`\n","* Из-за смены criterion, вы уже должны подавать на вход criterion ни одно число, а logits для каждого класса. Перед подачей logits вы можете отфильтровать предсказания для класса padding. В этом пункте вам придется изменять файл `train_model.py`, а именно функциии `train` и `evaluate`.\n","* Используйте два варианта обучения: c весами в `CrossEntropyLoss` и без\n","* Также сравните `inference`, когда вы ранжируете logits, и когды вы выбирате предложения, у котрых logits > 0, в двух вариантах обучения. \n","* Реализуйте дополнительно характеристику предложения `novelty`. Как влияет добавление `novelty` на качество summary?\n","* Постарайтесь улучшить качество модели, полученной на семинаре: $BLEU \\approx 0.45$"]},{"cell_type":"code","metadata":{"id":"iW7iS76KeEdO"},"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","from torch.nn.utils.rnn import pack_padded_sequence as pack\n","from torch.nn.utils.rnn import pad_packed_sequence as unpack\n","\n","class SentenceEncoderRNN(nn.Module):\n","    def __init__(self, input_size, embedding_dim, hidden_size, n_layers=3, dropout=0.3, bidirectional=True):\n","        super().__init__()\n","\n","        num_directions = 2 if bidirectional else 1\n","        assert hidden_size % num_directions == 0\n","        hidden_size = hidden_size // num_directions\n","\n","        self.embedding_dim = embedding_dim\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","\n","        self.embedding_layer = nn.Embedding(input_size, embedding_dim)\n","        self.rnn_layer = nn.LSTM(embedding_dim, hidden_size, n_layers, dropout=dropout, bidirectional=bidirectional, batch_first=True)\n","        self.dropout_layer = nn.Dropout(dropout)\n","\n","    def forward(self, inputs, hidden=None):\n","        embedded = self.dropout_layer(self.embedding_layer(inputs))\n","        outputs, _ = self.rnn_layer(embedded, hidden)\n","        sentences_embeddings = torch.mean(outputs, 1)\n","        # [batch_size, hidden_size]\n","        return sentences_embeddings\n","\n","class SentenceTaggerRNN(nn.Module):\n","    def __init__(self,\n","                 vocabulary_size,\n","                 use_content=True,\n","                 use_salience=True,\n","                 use_novelty=True,\n","                 token_embedding_dim=128,\n","                 sentence_encoder_hidden_size=256,\n","                 hidden_size=256,\n","                 bidirectional=True,\n","                 sentence_encoder_n_layers=2,\n","                 sentence_encoder_dropout=0.3,\n","                 sentence_encoder_bidirectional=True,\n","                 n_layers=2,\n","                 dropout=0.3):\n","        \n","        super().__init__()\n","\n","        num_directions = 2 if bidirectional else 1\n","        assert hidden_size % num_directions == 0\n","        hidden_size = hidden_size // num_directions\n","\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        self.dropout = dropout\n","        self.bidirectional = bidirectional\n","\n","        self.sentence_encoder = SentenceEncoderRNN(vocabulary_size, token_embedding_dim,\n","                                                   sentence_encoder_hidden_size, sentence_encoder_n_layers, \n","                                                   sentence_encoder_dropout, sentence_encoder_bidirectional)\n","        \n","        self.rnn_layer = nn.LSTM(sentence_encoder_hidden_size, hidden_size, n_layers, dropout=dropout,\n","                           bidirectional=bidirectional, batch_first=True)\n","        \n","        self.dropout_layer = nn.Dropout(dropout)\n","        self.content_linear_layer = nn.Linear(hidden_size * 2, 1)\n","        self.document_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n","        self.salience_linear_layer = nn.Linear(hidden_size * 2, hidden_size * 2)\n","        self.novelty_linear_layer = # YOUR CODE\n","        self.tanh_layer = nn.Tanh()\n","\n","        self.use_content = use_content\n","        self.use_salience = use_salience\n","        self.use_novelty = use_novelty\n","\n","    def forward(self, inputs, hidden=None):\n","        # parameters of the probability\n","        content = 0\n","        salience = 0\n","        novelty = 0\n","\n","        # [batch_size, seq num, seq_len]\n","        batch_size = inputs.size(0)\n","        sentences_count = inputs.size(1)\n","        tokens_count = inputs.size(2)\n","        inputs = inputs.reshape(-1, tokens_count)\n","        # [batch_size * seq num, seq_len]\n","\n","        embedded_sentences = self.sentence_encoder(inputs)\n","        embedded_sentences = self.dropout_layer(embedded_sentences.reshape(batch_size, sentences_count, -1))\n","        # [batch_size *  seq num, seq_len, hidden_size] -> [batch_size, seq num, hidden_size]\n","\n","        outputs, _ = self.rnn_layer(embedded_sentences, hidden)\n","        # [batch_size, seq num, hidden_size]\n","\n","        document_embedding = self.tanh_layer(self.document_linear_layer(torch.mean(outputs, 1)))\n","        # [batch_size, hidden_size]\n","\n","        # W * h^T\n","        if self.use_content:\n","            content = self.content_linear_layer(outputs).squeeze(2) # 1-representation\n","        # [batch_size, seq num]\n","\n","        # h^T * W * d\n","        if self.use_salience:\n","            salience = torch.bmm(outputs, self.salience_linear_layer(document_embedding).unsqueeze(2)).squeeze(2) # 2-representation\n","        # [batch_size, seq num, hidden_size] * [batch_size, hidden_size, 1] = [batch_size, seq num, ]\n","\n","        if self.use_novelty:\n","            # at every step add novelty to prediction of the sentence\n","            predictions = content + salience\n","            \n","            # 0) initialize summary_representation and novelty by zeros\n","            # YOUR CODE\n","\n","            for sentence_num in range(sentences_count):\n","\n","                # 1) take sentence_num_state from outputs(representation of the sentence with number sentence_num)\n","                # 2) calculate novelty for current sentence\n","                # 3) add novelty to predictions\n","                # 4) calculcate probability for current sentence\n","                # 5) add sentence_num_state with the weight which is equal to probability to summary_representation\n","\n","                # YOUR CODE\n","\n","        return content + salience + novelty"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QxpL3AtOrctD"},"source":["## Model\n","$P\\left(y_{j} = 1 \\mid \\mathbf{h}_{j}, \\mathbf{s}_{j}, \\mathbf{d}\\right)=\\sigma\\left(W_{c} \\mathbf{h}_{j} + \\mathbf{h}_{j}^{T} W_{s} \\mathbf{d}\\right)$\n","--------------------"]},{"cell_type":"code","metadata":{"id":"-QC1ZmuQfB7f"},"source":["vocab_size = len(vocabulary)\n","model = SentenceTaggerRNN(vocab_size).to(device)\n","\n","params_count = np.sum([p.numel() for p in model.parameters() if p.requires_grad])\n","print(\"Trainable params: {}\".format(params_count))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f9Q3aFHhgsB4"},"source":["for name, param in model.named_parameters():\n","    print(f\"{name}: {param.numel()}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"05s8UWh81cjG"},"source":["!gdown https://drive.google.com/uc?id=1MiS_iczALcyF7zGDPY6niyeD82P0_PBH -O train_model.py\n","import train_model\n","import imp \n","imp.reload(train_model)\n","from train_model import train_with_logs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwrhG4v71yts"},"source":["N_EPOCHS = 3\n","CLIP = 1\n","\n","def train(use_class_weights, N_EPOCHS, CLIP, lr=1e-3):\n","    optimizer = optim.Adam(model.parameters(), lr)\n","    if use_class_weights:\n","        # weights depend on the number of objects of class 0 and 1\n","        # YOUR CODE\n","        criterion = nn.CrossEntropyLoss(weight=weights)\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","    train_with_logs(model, train_iterator, val_iterator, optimizer, criterion, N_EPOCHS, CLIP)\n","\n","train(True, N_EPOCHS, CLIP)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vuv4Sh2Vj5Yr"},"source":["## Inference"]},{"cell_type":"code","metadata":{"id":"wZ96X37bb_PV"},"source":["from train_model import punct_detokenize, postprocess\n","\n","def inference_summarunner(model, iterator, top_k=3):\n","\n","    references = []\n","    predictions = []\n","\n","    model.eval()\n","    for batch in test_iterator:\n","\n","        logits = model(batch['inputs'])\n","        sum_in = torch.argsort(logits, dim=1)[:, -top_k:]\n","        \n","        for i in range(len(batch['outputs'])):\n","\n","            summary = batch['records'][i]['summary'].lower()\n","            pred_summary = ' '.join([batch['records'][i]['sentences'][ind] for ind in sum_in.sort(dim=1)[0][i]])\n","\n","            summary, pred_summary = postprocess(summary, pred_summary)\n","\n","            references.append(summary)\n","            predictions.append(pred_summary)\n","\n","    calc_scores(references, predictions)\n","\n","model.load_state_dict(torch.load('best-val-model.pt'))\n","inference_summarunner(model, test_iterator, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGc4IV8dzyBP"},"source":["## Вывод:"]}]}