{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers datasets tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ. Transformers.\n",
    "\n",
    "–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ –æ—á–µ—Ä–µ–¥–Ω–æ–µ –¥–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ, –Ω–æ —Ç–µ–ø–µ—Ä—å —Ç—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏—à—å—Å—è —Å –º–æ–¥–µ–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –∏ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π `HuggingFaceü§ó`. –í —ç—Ç–æ–º –∑–∞–¥–∞–Ω–∏–∏ –±—É–¥–µ—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ —Ä–µ—à–∏—Ç—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥–µ–ª–∏ `GPT2` –ø—Ä–æ—Å—Ç—É—é –∑–∞–¥–∞—á—É (–∞–Ω–∞–ª–∏–∑ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç–∞) –∏ —Å–¥–µ–ª–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç –≤–Ω–∏–º–∞–Ω–∏—è. –ü—Ä–∏—Å—Ç—É–ø–∏–º!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification, GPT2TokenizerFast, GPT2Config\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞—Ç–∞—Å–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–µ–≥–æ–¥–Ω—è ‚Äì —Ç–µ–∫—Å—Ç—ã –∏–∑ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —Ç–≤–∏—Ç—Ç–µ—Ä–∞. –û–Ω–∏ —É–∂–µ –ø–æ—á–∏—â–µ–Ω–Ω—ã –æ—Ç –Ω–∏–∫–Ω–µ–π–º–æ–≤, –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∏ –ø—Ä–æ—á–µ–≥–æ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset = load_dataset(\"emotion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏, –∏–∑ —á–µ–≥–æ —Å–æ—Å—Ç–æ–∏—Ç `emotion_dataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset[\"train\"][\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dataset[\"train\"][\"label\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(emotion_dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞ –≤ —Ç–æ–∫–µ–Ω—ã –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–π BPE-—Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token # –£ gpt2 –Ω–µ—Ç pad —Ç–æ–∫–µ–Ω–æ–≤. –í–º–µ—Å—Ç–æ –Ω–∏—Ö –≤–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ç–æ–∫–µ–Ω–∞–º–∏ –∫–æ–Ω—Ü–∞ —Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤—å –∫–ª–∞—Å—Å, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç, —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –∏ –∏–º—è –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–π —á–∞—Å—Ç–∏ (`train`, `validation`, `test`). –ò—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "P.S. –ü–æ—Å–º–æ—Ç—Ä–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä ([docs](https://huggingface.co/transformers/main_classes/tokenizer.html)) –∏ –ø–æ–¥—É–º–∞–π, –∫–∞–∫ –µ–≥–æ –Ω–∞–¥–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–º–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç —Å –Ω–∞–ø–∏—Å–∞–Ω–∏–µ–º –¥–∞—Ç–∞—Å–µ—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(emotion_dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(emotion_dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus(emotion_dataset[\"train\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus(emotion_dataset[\"train\"][\"text\"][0], return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus(\n",
    "    emotion_dataset[\"train\"][\"text\"][0], \n",
    "    max_length=128, # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞\n",
    "    padding=\"max_length\", # –Ω–∞–¥–æ –ª–∏ –¥–æ–±–∞–≤–ª—è—Ç—å –ø–∞–¥–¥–∏–Ω–≥ –≤ –∫–æ–Ω—Ü–µ?\n",
    "    return_tensors=\"pt\", # –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç pytorch —Ç–µ–Ω–∑–æ—Ä—ã\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ï—Å–ª–∏ –Ω–∞–¥–æ, –ø–æ–ø—Ä–∞–∫—Ç–∏–∫—É–π—Å—è —Ä–∞–±–æ—Ç–∞—Ç—å —Å —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä–æ–º –∑–¥–µ—Å—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, part, dataset=emotion_dataset, tokenizer=tokenizer, max_length=128):\n",
    "        self.part = part\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.labels = np.unique(dataset[part][\"label\"])\n",
    "        self.label2num = {l: num for num, l in enumerate(self.labels)}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return dict with tokens, attention_mask and label\n",
    "        \"\"\"\n",
    "        text = ...\n",
    "        label = ...\n",
    "        \n",
    "        tokenizer_output = ...\n",
    "        target = self.label2num[label]\n",
    "        return {\n",
    "            \"input_ids\": ..., \n",
    "            \"mask\": ...,\n",
    "            \"target\": target\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return length of dataset\n",
    "        \"\"\"\n",
    "        return len(self.dataset[self.part])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–π `train`, `validation` –∏ `test` —á–∞—Å—Ç–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞. –ó–∞–≥—Ä—É–∑–∏ –∏—Ö –≤ `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ...\n",
    "valid_dataset = ... # validation\n",
    "test_dataset = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64 # –ó–∞–¥–∞–π batch_size\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    ...\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    ...\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ù–∞—á–Ω–µ–º —Å –Ω—É–ª—è.\n",
    "\n",
    "–ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å –Ω—É–ª—è —Ä–µ—à–∞—Ç—å –¥–∞–Ω–Ω—É—é –∑–∞–¥–∞—á—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config.from_pretrained(\n",
    "    \"distilgpt2\", # distilgpt2 ‚Äì¬†—É–º–µ–Ω—å—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ gpt2\n",
    "    output_attentions=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_labels=8\n",
    ")\n",
    "model_0 = GPT2ForSequenceClassification(config=config).to(device) # GPT2 –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥–≥–æ—Ç–æ–≤—å –æ–ø—Ç–∏–º–∞–π–∑–µ—Ä –∏ –∫—Ä–∏—Ç–µ—Ä–∏–π:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5 # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π learning rate. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ –∏–ª–∏ –º–µ–Ω—å—à–µ :)\n",
    "\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "# scheduler = ... # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —à–µ–¥—É–ª–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –Ω–∞ —Ç–≤–æ–µ —É—Å–º–æ—Ç—Ä–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–æ–¥–µ–ª—å ([docs](https://huggingface.co/transformers/model_doc/gpt2.html#gpt2forsequenceclassification)), –µ—Å–ª–∏ –≤ –Ω–µ—ë –ø–æ–¥–∞—Ç—å –¥–∞–Ω–Ω—ã–µ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ... # –ü–æ–ª—É—á–∏ —Ç–æ–∫–µ–Ω—ã –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "mask = ... # –ü–æ–ª—É—á–∏ –º–∞—Å–∫—É –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "\n",
    "model_0(...) # –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∞—É—Ç–ø—É—Ç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∏ –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é `train_dataset`, –ø—Ä–æ–≤–µ—Ä—è–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é `valid_dataset` –∏ –ø–æ–ª—É—á–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å –ø–æ–º–æ—â—å—é `test_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for e in range(num_epochs):\n",
    "    model_0.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        ...\n",
    "        \n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    model_0.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            ...\n",
    "    print(f\"Train Loss: {train_loss / len(train_loader)},\"\n",
    "          f\"Valid Loss: {valid_loss / len(valid_loader)},\"\n",
    "          f\"Valid Acc: {valid_acc / len(valid_loader)}\")\n",
    "        \n",
    "# Testing\n",
    "test_acc = 0\n",
    "model_0.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ...\n",
    "print(f\"Test Acc: {test_acc / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ–ª—É—á–∏–ª –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —ç—Ç–æ–π –∑–∞–¥–∞—á–∏, –ø–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è. –ù–∞—à–µ–ª –ª–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ –≤ –Ω–∏—Ö?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_matrixes(model, tokenizer, text, device=device):\n",
    "    inp = list(filter(lambda x: x != tokenizer.sep_token_id, tokenizer.encode(text)))\n",
    "    inp = torch.tensor(inp, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    attn_tensors = model(inp)[-1]\n",
    "    seq = [tokenizer.decode(x) for x in inp[0].tolist()]\n",
    "    attn = []\n",
    "    for i in range(len(attn_tensors)):\n",
    "        attn_layer = []\n",
    "        for j in range(attn_tensors[i].size(1)):\n",
    "            attn_layer.append(attn_tensors[i][0, j].cpu().detach().numpy())\n",
    "        attn.append(np.array(attn_layer))\n",
    "    \n",
    "    return np.array(attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(seq, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions)\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels(['']+seq, rotation=90, fontsize=16)\n",
    "    ax.set_yticklabels(['']+seq, fontsize=16)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # –í—ã–±–µ—Ä–∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attns = get_ettention_matrixes(model_0, tokenizer, text)\n",
    "show_attention(seq, attn[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –¥—Ä—É–≥–æ–π –ø–æ–¥—Ö–æ–¥: –∑–∞–≥—Ä—É–∑–∏–º –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä–∞—è –æ–±—É—á–∞–ª–∞—Å—å —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É Language Modeling. –ü–æ—Å–º–æ—Ç—Ä–∏–º, –ø–æ–ª—É—á–∏–º –ª–∏ –º—ã –ø—Ä–∏—Ä–æ—Å—Ç –≤ –∫–∞—á–µ—Å—Ç–≤–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = GPT2ForSequenceClassification.from_pretrained(\n",
    "    \"distilgpt2\", \n",
    "    output_attentions=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    num_labels=8\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5 # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º—ã–π learning rate. –û–Ω –º–æ–∂–µ—Ç –±—ã—Ç—å –±–æ–ª—å—à–µ –∏–ª–∏ –º–µ–Ω—å—à–µ :)\n",
    "\n",
    "optimizer = ...\n",
    "criterion = ...\n",
    "# scheduler = ... # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —à–µ–¥—É–ª–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π. –≠—Ç–æ –Ω–∞ —Ç–≤–æ–µ —É—Å–º–æ—Ç—Ä–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –Ω–∏—á–µ–º –Ω–µ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª—É—á–∞—è, –ø–æ—ç—Ç–æ–º—É —Å—Ä–∞–∑—É –ø—Ä–∏—Å—Ç—É–ø–∞–µ–º –∫ –æ–±—É—á–µ–Ω–∏—é:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Train loop\n",
    "for e in range(num_epochs):\n",
    "    model_1.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_loader):\n",
    "        ...\n",
    "        \n",
    "    valid_loss = 0\n",
    "    valid_acc = 0\n",
    "    model_1.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            ...\n",
    "    print(f\"Train Loss: {train_loss / len(train_loader)},\"\n",
    "          f\"Valid Loss: {valid_loss / len(valid_loader)},\"\n",
    "          f\"Valid Acc: {valid_acc / len(valid_loader)}\")\n",
    "        \n",
    "# Testing\n",
    "test_acc = 0\n",
    "model_1.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ...\n",
    "print(f\"Test Acc: {test_acc / len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å—Ç—å –ª–∏ –ø—Ä–∏—Ä–æ—Å—Ç –∫–∞—á–µ—Å—Ç–≤–∞ –∏–ª–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è?\n",
    "\n",
    "–ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∫–∞—Ä—Ç—ã –≤–Ω–∏–º–∞–Ω–∏—è. –ï—Å—Ç—å –ª–∏ –æ—Ç–ª–∏—á–∏–µ –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–ª—É—á–∞—è?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ... # –í—ã–±–µ—Ä–∏ —Ç–µ–∫—Å—Ç –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞\n",
    "tokens = tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attns = get_ettention_matrixes(model, tokenizer, text)\n",
    "show_attention(seq, attn[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û—Ç—á–µ—Ç\n",
    "\n",
    "–ü–æ–∫–∞–∂–∏ –∑–¥–µ—Å—å, —á—Ç–æ —Ç—ã –≤—ã–ø–æ–ª–Ω–∏–ª –ø–æ —ç—Ç–æ–π —Ä–∞–±–æ—Ç–µ. –û—Ç–≤–µ—Ç—å –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–æ–ø—Ä–æ—Å–æ–≤:\n",
    "- –ö–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –æ–∫–∞–∑–∞–ª—Å—è –ª—É—á—à–µ? \n",
    "- –ù–∞ –∫–∞–∫–∏–µ —Å–ª–æ–≤–∞ –º–æ–¥–µ–ª—å –±–æ–ª—å—à–µ–≥–æ –≤—Å–µ–≥–æ –æ–±—Ä–∞—â–∞–ª–∞ –≤–Ω–∏–º–∞–Ω–∏–µ?\n",
    "- –ù–∞ –∫–∞–∫–∏—Ö —Å–ª–æ—è—Ö/–≥–æ–ª–æ–≤–∞—Ö –º–æ–¥–µ–ª—å –æ–±—Ä–∞—â–∞–ª–∞ –≤–Ω–∏–º–∞–Ω–∏–µ?\n",
    "\n",
    "< —Ç–≤–æ–π –æ—Ç—á–µ—Ç/–æ—Ç–≤–µ—Ç—ã >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
