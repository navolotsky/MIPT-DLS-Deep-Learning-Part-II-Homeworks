{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ot3c4fjZwC4T"
   },
   "source": [
    "<img src=\"https://s8.hostingkartinok.com/uploads/images/2018/08/308b49fcfbc619d629fe4604bceb67ac.jpg\" width=500, height=450>\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2JdzEXmwRU5"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fc8iHXIVwDwj"
   },
   "source": [
    "***Some parts of the notebook are almost the copy of [ mmta-team course](https://github.com/mmta-team/mmta_fall_2020). Special thanks to mmta-team for making them publicly available. [Original notebook](https://github.com/mmta-team/mmta_fall_2020/blob/master/tasks/01_word_embeddings/task_word_embeddings.ipynb).***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7D0wm5jt6j0U"
   },
   "source": [
    "<b> Прочитайте семинар, пожалуйста, для успешного выполнения домашнего задания. В конце ноутка напишите свой вывод. Работа без вывода оценивается ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student: Oleg Navolotsky / Наволоцкий Олег  \n",
    "Stepik: https://stepik.org/users/2403189  \n",
    "Telegram: [@mehwhatever0](https://t.me/mehwhatever0)\n",
    "\n",
    "Some used software versions:\n",
    "- NumPy 1.18.5\n",
    "- NLTK 3.5\n",
    "- Gensim 3.8.3\n",
    "- Python 3.8.3 (default, Jul  2 2020, 17:30:36) \\[MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\n",
    "- Windows 10 Pro 1909, build 18363.535\n",
    "\n",
    "Hardware:\n",
    "- i5 2500 8 gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "def enable_reproducibility(seed=SEED):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BIWqBuEa6j0b"
   },
   "source": [
    "## Задача поиска схожих по смыслу предложений"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUkwMPLA6j0g"
   },
   "source": [
    "Мы будем ранжировать вопросы [StackOverflow](https://stackoverflow.com) на основе семантического векторного представления "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNRXIEfu5a3Q"
   },
   "source": [
    "До этого в курсе не было речи про задачу ранжировния, поэтому введем математическую формулировку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uS9FwWNd5a3S"
   },
   "source": [
    "## Задача ранжирования(Learning to Rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wdwY9-f75a3T"
   },
   "source": [
    "* $X$ - множество объектов\n",
    "* $X^l = \\{x_1, x_2, ..., x_l\\}$ - обучающая выборка\n",
    "<br>На обучающей выборке задан порядок между некоторыми элементами, то есть нам известно, что некий объект выборки более релевантный для нас, чем другой:\n",
    "* $i \\prec j$ - порядок пары индексов объектов на выборке $X^l$ c индексами $i$ и $j$\n",
    "### Задача:\n",
    "построить ранжирующую функцию $a$ : $X \\rightarrow R$ такую, что\n",
    "$$i \\prec j \\Rightarrow a(x_i) < a(x_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG2IGBsh5a3U"
   },
   "source": [
    "<img src=\"https://d25skit2l41vkl.cloudfront.net/wp-content/uploads/2016/12/Featured-Image.jpg\" width=500, height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQk_rolFwT_h"
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUe1PGXn6j0l"
   },
   "source": [
    "Будем использовать предобученные векторные представления слов на постах Stack Overflow.<br>\n",
    "[A word2vec model trained on Stack Overflow posts](https://github.com/vefstathiou/SO_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mYkI54Y-rk7a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'SO_vectors_200.bin' already there; not retrieving.\n",
      "C:\\Users\\user0\\anaconda3\\envs\\custom\\lib\\site-packages\\IPython\\utils\\_process_win32.py:145: ResourceWarning: unclosed file <_io.BufferedWriter name=4>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\user0\\anaconda3\\envs\\custom\\lib\\site-packages\\IPython\\utils\\_process_win32.py:145: ResourceWarning: unclosed file <_io.BufferedReader name=5>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "C:\\Users\\user0\\anaconda3\\envs\\custom\\lib\\site-packages\\IPython\\utils\\_process_win32.py:145: ResourceWarning: unclosed file <_io.BufferedReader name=6>\n",
      "  return process_handler(cmd, _system_body)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "!wget -nc https://zenodo.org/record/1199620/files/SO_vectors_200.bin?download=1 -O SO_vectors_200.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "O8YJTOYv6j0s"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user0\\anaconda3\\envs\\custom\\lib\\site-packages\\scipy\\sparse\\sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "wv_embeddings = KeyedVectors.load_word2vec_format(\"SO_vectors_200.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIcT_g-C6j1E"
   },
   "source": [
    "#### Как пользоваться этими векторами?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWO5SPDY6j1G"
   },
   "source": [
    "Посмотрим на примере одного слова, что из себя представляет embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KeSBlQfk6j1J",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 (200,)\n"
     ]
    }
   ],
   "source": [
    "word = 'dog'\n",
    "if word in wv_embeddings:\n",
    "    print(wv_embeddings[word].dtype, wv_embeddings[word].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "T4Eq-D1qxpMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of words: 1787145\n"
     ]
    }
   ],
   "source": [
    "print(f\"Num of words: {len(wv_embeddings.index2word)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZT6NTCys6j1Q"
   },
   "source": [
    "Найдем наиболее близкие слова к слову `dog`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n08z2PjMwC5o"
   },
   "source": [
    "#### Вопрос 1:\n",
    "* Входит ли слов `cat` топ-5 близких слов к слову `dog`? Какое место? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nYwVz0xG6j1U",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: не входит.\n"
     ]
    }
   ],
   "source": [
    "for i, (word, _) in enumerate(wv_embeddings.similar_by_word('dog', topn=5)):\n",
    "    if word == 'cat':\n",
    "        print(f\"Входит: {i} место.\")\n",
    "        break\n",
    "else:\n",
    "    print(\"Ответ: не входит.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai48-5vv6j1d"
   },
   "source": [
    "### Векторные представления текста\n",
    "\n",
    "Перейдем от векторных представлений отдельных слов к векторным представлениям вопросов, как к **среднему** векторов всех слов в вопросе. Если для какого-то слова нет предобученного вектора, то его нужно пропустить. Если вопрос не содержит ни одного известного слова, то нужно вернуть нулевой вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EhNuxBJd6j1f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# you can use your tokenizer\n",
    "# for example, from nltk.tokenize import WordPunctTokenizer\n",
    "class MyTokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return re.findall(r'\\w+', text)\n",
    "\n",
    "tokenizer = MyTokenizer().tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, tokenizer, dim=200):\n",
    "    \"\"\"\n",
    "        question: строка\n",
    "        embeddings: наше векторное представление\n",
    "        dim: размер любого вектора в нашем представлении\n",
    "        \n",
    "        return: векторное представление для вопроса\n",
    "    \"\"\"\n",
    "    assert embeddings.vectors.shape[-1] == dim\n",
    "    res_vec = np.zeros_like(embeddings.vectors[0])\n",
    "    num = 0\n",
    "    for word in tokenizer(question):\n",
    "        if word in embeddings:\n",
    "            res_vec += embeddings[word]\n",
    "            num += 1\n",
    "    return res_vec / num if num > 0 else res_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5Q_4j7r6j1u"
   },
   "source": [
    "Теперь у нас есть метод для создания векторного представления любого предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsJSNkhm6j1y"
   },
   "source": [
    "#### Вопрос 2:\n",
    "* Какая третья(с индексом 2) компонента вектора предложения `I love neural networks` (округлите до 2 знаков после запятой)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "a62r11cT6j10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: -1.29.\n"
     ]
    }
   ],
   "source": [
    "print(f'Ответ: {question_to_vec(\"I love neural networks\", wv_embeddings, tokenizer)[2].round(2):.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y60z4t6W6j16"
   },
   "source": [
    "### Оценка близости текстов\n",
    "\n",
    "Представим, что мы используем идеальные векторные представления слов. Тогда косинусное расстояние между дублирующими предложениями должно быть меньше, чем между случайно взятыми предложениями. \n",
    "\n",
    "Сгенерируем для каждого из $N$ вопросов $R$ случайных отрицательных примеров и примешаем к ним также настоящие дубликаты. Для каждого вопроса будем ранжировать с помощью нашей модели $R + 1$ примеров и смотреть на позицию дубликата. Мы хотим, чтобы дубликат был первым в ранжированном списке.\n",
    "\n",
    "#### Hits@K\n",
    "Первой простой метрикой будет количество корректных попаданий для какого-то $K$:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [rank\\_q_i^{'} \\le K],$$\n",
    "* $\\begin{equation*}\n",
    "[x < 0 ] \\equiv \n",
    " \\begin{cases}\n",
    "   1, &x < 0\\\\\n",
    "   0, &x \\geq 0\n",
    " \\end{cases}\n",
    "\\end{equation*}$ - индикаторная функция\n",
    "* $q_i$ - $i$-ый вопрос\n",
    "* $q_i^{'}$ - его дубликат\n",
    "* $rank\\_q_i^{'}$ - позиция дубликата в ранжированном списке ближайших предложений для вопроса $q_i$.\n",
    "\n",
    "#### DCG@K\n",
    "Второй метрикой будет упрощенная DCG метрика, учитывающая порядок элементов в списке путем домножения релевантности элемента на вес равный обратному логарифму номера позиции::\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank\\_q_i^{'})}\\cdot[rank\\_q_i^{'} \\le K],$$\n",
    "С такой метрикой модель штрафуется за большой ранк корректного ответа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHCnH-jw6j18"
   },
   "source": [
    "#### Вопрос 3:\n",
    "* Максимум `Hits@47 - DCG@1`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ: **1**.\n",
    "\n",
    "$Hits@K$ не учитывает позицию, а значит её значение максимально, если для кажого из $N$ вопросов позиция его дубликата $\\le K$. Минимальное значение **DCG@K** — ноль, достигается при рангах все дубликатов $\\gt K$. Для данных $Hits@47$ и $DCG@1$ теоретически достижимое значение разности $Hits@47 - DCG@1$ равно 1. Модель должна быть такой, что ранги дубликатов для всех $N$ вопросов удовлетворяют $2 \\le rank\\_q_{i}^{'} \\le 47$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tFemBkP6j1-"
   },
   "source": [
    "<img src='https://hsto.org/files/1c5/edf/dee/1c5edfdeebce4b71a86bdf986d9f88f2.jpg' width=400, height=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sUSxk866j1_"
   },
   "source": [
    "#### Пример оценок\n",
    "\n",
    "Вычислим описанные выше метрики для игрушечного примера. \n",
    "Пусть\n",
    "* $N = 1$, $R = 3$\n",
    "* <font color='green'>\"Что такое python?\"</font> - вопрос $q_1$\n",
    "* <font color='red'>\"Что такое язык python?\"</font> - его дубликат $q_i^{'}$\n",
    "\n",
    "Пусть модель выдала следующий ранжированный список кандидатов:\n",
    "\n",
    "1. \"Как изучить с++?\"\n",
    "2. <font color='red'>\"Что такое язык python?\"</font>\n",
    "3. \"Хочу учить Java\"\n",
    "4. \"Не понимаю Tensorflow\"\n",
    "\n",
    "$\\Rightarrow rank\\_q_i^{'} = 2$\n",
    "\n",
    "Вычислим метрику *Hits@K* для *K = 1, 4*:\n",
    "\n",
    "- [K = 1] $\\text{Hits@1} =  [rank\\_q_i^{'} \\le 1)] = 0$\n",
    "- [K = 4] $\\text{Hits@4} =  [rank\\_q_i^{'} \\le 4] = 1$\n",
    "\n",
    "Вычислим метрику *DCG@K* для *K = 1, 4*:\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{\\log_2(1+2)}\\cdot[2 \\le 1] = 0$\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{\\log_2(1+2)}\\cdot[2 \\le 4] = \\frac{1}{\\log_2{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4L6HJJC6j2B"
   },
   "source": [
    "#### Вопрос 4:\n",
    "* Вычислите `DCG@10`, если $rank\\_q_i^{'} = 9$(округлите до одного знака после запятой)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ: $\\text{DCG@10} = \\frac{1}{\\log_2(1+9)}\\cdot[9 \\le 10] = \\frac{1}{\\log_2{10}} \\approx$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≈ 0.3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "print('\\u2248', round(1 / math.log2(10), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5xWOORI6j2F"
   },
   "source": [
    "### HITS\\_COUNT и DCG\\_SCORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1q9WQOx6j2H"
   },
   "source": [
    "Каждая функция имеет два аргумента: $dup\\_ranks$ и $k$. $dup\\_ranks$ является списком, который содержит рейтинги дубликатов(их позиции в ранжированном списке). Например, $dup\\_ranks = [2]$ для примера, описанного выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F5VwySUB6j2J"
   },
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list индексов дубликатов\n",
    "        result: вернуть  Hits@k\n",
    "    \"\"\"\n",
    "    dup_ranks = np.array(dup_ranks)\n",
    "    assert np.all(dup_ranks > 0)\n",
    "    hits_value = (dup_ranks <= k).mean()\n",
    "    return hits_value    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "82hQaxCH6j2R"
   },
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list индексов дубликатов\n",
    "        result: вернуть DCG@k\n",
    "    \"\"\"\n",
    "    dup_ranks = np.array(dup_ranks)\n",
    "    assert np.all(dup_ranks > 0)\n",
    "    dcg_value = np.mean((dup_ranks <= k) / np.log2(1 + dup_ranks))\n",
    "    return dcg_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PcwHeXN26j2Y"
   },
   "source": [
    "Протестируем функции. Пусть $N = 1$, то есть один эксперимент. Будем искать копию вопроса и оценивать метрики."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fjISmOEW6j2h"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gLa_Wqfh6j2m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dup_ranks = [2]\n",
      "Ваш ответ HIT: [0.0, 1.0, 1.0, 1.0]\n",
      "Ваш ответ DCG: [0.0, 0.63093, 0.63093, 0.63093]\n"
     ]
    }
   ],
   "source": [
    "copy_answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "\n",
    "# наги кандидаты\n",
    "candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                       \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                       \"NSLog array description not memory address\",\n",
    "                       \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "# dup_ranks — позиции наших копий, так как эксперимент один, то этот массив длины 1\n",
    "dup_ranks = [cands.index(q) + 1 for q, cands in zip(copy_answers, candidates_ranking)]\n",
    "print(f\"{dup_ranks = }\")\n",
    "\n",
    "# вычисляем метрику для разных k\n",
    "print('Ваш ответ HIT:', [hits_count(dup_ranks, k) for k in range(1, 5)])\n",
    "print('Ваш ответ DCG:', [round(dcg_score(dup_ranks, k), 5) for k in range(1, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoHC3YoQ6j2t"
   },
   "source": [
    "У вас должно получиться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B0NFWq4f6j2u",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HITS</th>\n",
       "      <td>0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DCG</th>\n",
       "      <td>0</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.63093</td>\n",
       "      <td>0.63093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      1        2        3        4\n",
       "HITS  0  1.00000  1.00000  1.00000\n",
       "DCG   0  0.63093  0.63093  0.63093"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correct_answers - метрика для разных k\n",
    "correct_answers = pd.DataFrame([[0, 1, 1, 1], [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]],\n",
    "                               index=['HITS', 'DCG'], columns=range(1,5))\n",
    "correct_answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHZqgDTo6j0i"
   },
   "source": [
    "### Данные\n",
    "[arxiv link](https://drive.google.com/file/d/1QqT4D0EoqJTy7v9VrNCYD-m964XZFR7_/edit)\n",
    "\n",
    "`train.tsv` - выборка для обучения.<br> В каждой строке через табуляцию записаны: **<вопрос>, <похожий вопрос>**\n",
    "\n",
    "`validation.tsv` - тестовая выборка.<br> В каждой строке через табуляцию записаны: **<вопрос>, <похожий вопрос>, <отрицательный пример 1>, <отрицательный пример 2>, ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "jKVK2lDGvrIe"
   },
   "outputs": [],
   "source": [
    "# !unzip stackoverflow_similar_questions.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hil2UsUG6j22"
   },
   "source": [
    "Считайте данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "B4EBho8s6j26"
   },
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    with open(filename, encoding='utf-8') as input_file:\n",
    "        return [line.strip().split('\\t') for line in input_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkTxY3Mk9_nG"
   },
   "source": [
    "Нам понадобиться только файл validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PTVB9Tnp6j29"
   },
   "outputs": [],
   "source": [
    "validation_data = read_corpus('./data/validation.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTHfL-9y6j3F"
   },
   "source": [
    "Кол-во строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z6ubXhIe6j3H",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3760"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kaOQblBy6j3M"
   },
   "source": [
    "Размер нескольких первых строк"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yRx6e-Pe6j3M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1001\n",
      "2 1001\n",
      "3 1001\n",
      "4 1001\n",
      "5 1001\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i + 1, len(validation_data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySQQp0oQt1Ep"
   },
   "source": [
    "### Ранжирование без обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iElEDhj-6j3R"
   },
   "source": [
    "Реализуйте функцию ранжирования кандидатов на основе косинусного расстояния. Функция должна по списку кандидатов вернуть отсортированный список пар (позиция в исходном списке кандидатов, кандидат). При этом позиция кандидата в полученном списке является его рейтингом (первый - лучший). Например, если исходный список кандидатов был [a, b, c], и самый похожий на исходный вопрос среди них - c, затем a, и в конце b, то функция должна вернуть список **[(2, c), (0, a), (1, b)]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "K02JARKr6j3T"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "1yP8wJWj6j3X"
   },
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, preprocessor, dim=200):\n",
    "    \"\"\"\n",
    "        question: строка\n",
    "        candidates: массив строк(кандидатов) [a, b, c]\n",
    "        result: пары (начальная позиция, кандидат) [(2, c), (0, a), (1, b)]\n",
    "    \"\"\"\n",
    "    q_vec_resh = question_to_vec(question, embeddings, preprocessor, dim).reshape(1, -1)\n",
    "    return sorted(\n",
    "        enumerate(candidates),\n",
    "        key=lambda i_c: 1 - cosine_similarity(\n",
    "            q_vec_resh,\n",
    "            question_to_vec(i_c[1], embeddings, preprocessor, dim).reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnBszTb76j3c"
   },
   "source": [
    "Протестируйте работу функции на примерах ниже. Пусть $N=2$, то есть два эксперимента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xvQgtP176j3h"
   },
   "outputs": [],
   "source": [
    "questions = ['converting string to list', 'Sending array via Ajax fails'] \n",
    "\n",
    "candidates = [['Convert Google results object (pure js) to Python object', # первый эксперимент\n",
    "               'C# create cookie from string and send it',\n",
    "               'How to use jQuery AJAX for an outside domain?'],\n",
    "              \n",
    "              ['Getting all list items of an unordered list in PHP',      # второй эксперимент\n",
    "               'WPF- How to update the changes in list item of a list',\n",
    "               'select2 not displaying search results']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "bPj1JGFi6j3m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'C# create cookie from string and send it'), (0, 'Convert Google results object (pure js) to Python object'), (2, 'How to use jQuery AJAX for an outside domain?')]\n",
      "\n",
      "[(1, 'WPF- How to update the changes in list item of a list'), (0, 'Getting all list items of an unordered list in PHP'), (2, 'select2 not displaying search results')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for question, q_candidates in zip(questions, candidates):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, tokenizer)\n",
    "        print(ranks, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm4cidj56j3q"
   },
   "source": [
    "Для первого экперимента вы можете полностью сравнить ваши ответы и правильные ответы. Но для второго эксперимента два ответа на кандидаты будут <b>скрыты</b>(*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "0LeKMIsn6j3s"
   },
   "outputs": [],
   "source": [
    "# # должно вывести\n",
    "# results = [[(1, 'C# create cookie from string and send it'),\n",
    "#             (0, 'Convert Google results object (pure js) to Python object'),\n",
    "#             (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "#            [(*, 'Getting all list items of an unordered list in PHP'), #скрыт\n",
    "#             (*, 'select2 not displaying search results'), #скрыт\n",
    "#             (*, 'WPF- How to update the changes in list item of a list')]] #скрыт"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1ttnIBe6j3x"
   },
   "source": [
    "Последовательность начальных индексов вы должны получить `для эксперимента 1`  1, 0, 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WQgYDWd6j3y"
   },
   "source": [
    "#### Вопрос 5:\n",
    "* Какую последовательность начальных индексов вы получили `для эксперимента 2`(перечисление без запятой и пробелов, например, `102` для первого эксперимента?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответ: 102."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPllOY-Y6j30"
   },
   "source": [
    "Теперь мы можем оценить качество нашего метода. Запустите следующие два блока кода для получения результата. Обратите внимание, что вычисление расстояния между векторами занимает некоторое время (примерно 10 минут). Можете взять для validation 1000 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "MAX_VAL_EX = 1000\n",
    "\n",
    "\n",
    "def make_validation(embeddings, tokenizer, validation_data, max_validation_examples=MAX_VAL_EX):\n",
    "    ranking = []\n",
    "    for i, line in tqdm(\n",
    "            enumerate(islice(validation_data, max_validation_examples)),\n",
    "            total=max_validation_examples):\n",
    "        q, *ex = line\n",
    "        ranks = rank_candidates(q, ex, embeddings, tokenizer)\n",
    "        ranking.append([r[0] for r in ranks].index(0) + 1)\n",
    "    return ranking\n",
    "\n",
    "\n",
    "def get_metrics(ranking, *, K=(1, 5, 10, 100, 500, 1000), show=True):\n",
    "    metrics = {}\n",
    "    for k in tqdm(K):\n",
    "        dcg = dcg_score(ranking, k)\n",
    "        hits = hits_count(ranking, k)\n",
    "        if show:\n",
    "            print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg, k, hits))\n",
    "        metrics[f\"DCG@{k}\"] = dcg\n",
    "        metrics[f\"Hits@{k}\"] = hits\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class UpdatableListValueDictionary(dict):\n",
    "    def update(self, other, **kwargs):\n",
    "        if hasattr(other, 'keys'):\n",
    "            for key in other.keys():\n",
    "                self[key] = other[key]\n",
    "        else:\n",
    "            for key, value in other:\n",
    "                self[key] = value\n",
    "        for key, value in kwargs.items():\n",
    "            self[key] = value\n",
    "    \n",
    "    def __setitem__(self, key, value):\n",
    "        if isinstance(value, list):\n",
    "            super().__setitem__(key, value)\n",
    "            return\n",
    "        if key in self:\n",
    "            self[key].append(value)\n",
    "        else:\n",
    "            super().__setitem__(key, [value])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = UpdatableListValueDictionary()\n",
    "so_embeddings = wv_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #1:\n",
    "default tokenizer & stackoverflow embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "nu7K4mis6j32"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e223b0af0394669abea8a6f4a9b3b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, tokenizer, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gDtS520v6j35",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebfbd1fa35bf45dbad3d96742e1aabc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.285 | Hits@   1: 0.285\n",
      "DCG@   5: 0.342 | Hits@   5: 0.393\n",
      "DCG@  10: 0.360 | Hits@  10: 0.449\n",
      "DCG@ 100: 0.406 | Hits@ 100: 0.679\n",
      "DCG@ 500: 0.431 | Hits@ 500: 0.879\n",
      "DCG@1000: 0.444 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#1: def tknz & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LL6_Rjg3InL8"
   },
   "source": [
    "### Эмбеддинги, обученные на корпусе похожих вопросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iNvbpR5gJIPz"
   },
   "outputs": [],
   "source": [
    "train_data = read_corpus('./data/train.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr281ZyEJfjT"
   },
   "source": [
    "\n",
    "Улучшите качество модели.<br>Склеим вопросы в пары и обучим на них модель Word2Vec из gensim. Выберите размер window. Объясните свой выбор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "def train_embeddings(data, *, window, min_count, size=200):\n",
    "    enable_reproducibility()\n",
    "    return Word2Vec(\n",
    "        data,  # data for model to train on\n",
    "        size=size,  # embedding vector size\n",
    "        window=window,\n",
    "        min_count=min_count  # consider words that occured at least given times\n",
    "    ).wv\n",
    "\n",
    "\n",
    "def prepare_data(data, preprocessor):\n",
    "    return map(preprocessor, (' '.join(doc) for doc in data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #2:\n",
    "default tokenizer & our own embeddings (default window (5), default min_count (5))\n",
    "\n",
    "Размер окна оставлен таким, какой он стоит по умолчанию в сигнатуре конструктора. Обычно в таких библиотеках если установлено некое значение аргумента по умолчанию, то оно уже достаточно оптимальное в большинстве случаев и не является тем, что нужно подбирать и оптимизировать в первую очередь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 28.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_2_embeddings_trained = train_embeddings(prepare_data(train_data, tokenizer), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "OQonbm4nMenD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3331cd0f3e004191aa063ecaad71bbfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_2_embeddings_trained, tokenizer, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "3kahBUPGMgGR"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473cab2070fc4148a5f183479c5cc564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.198 | Hits@   1: 0.198\n",
      "DCG@   5: 0.238 | Hits@   5: 0.274\n",
      "DCG@  10: 0.250 | Hits@  10: 0.313\n",
      "DCG@ 100: 0.290 | Hits@ 100: 0.516\n",
      "DCG@ 500: 0.324 | Hits@ 500: 0.783\n",
      "DCG@1000: 0.347 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#2: def tknz & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #3:\n",
    "default tokenizer + lowercase & stackoverflow embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return tokenizer(data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f95bf54e51b4460b3d07b638693e0c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4527e8757f87495eb3ec5c2745da3ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.415 | Hits@   1: 0.415\n",
      "DCG@   5: 0.502 | Hits@   5: 0.582\n",
      "DCG@  10: 0.525 | Hits@  10: 0.651\n",
      "DCG@ 100: 0.570 | Hits@ 100: 0.874\n",
      "DCG@ 500: 0.583 | Hits@ 500: 0.973\n",
      "DCG@1000: 0.586 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#3: def tknz + lowercase & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #4:\n",
    "default tokenizer + lowercase & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_4_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c70c2d6800e4e13be277d61a6efb4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_4_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe7ac85738249329d33fb117f2b591b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.261 | Hits@   1: 0.261\n",
      "DCG@   5: 0.315 | Hits@   5: 0.361\n",
      "DCG@  10: 0.332 | Hits@  10: 0.414\n",
      "DCG@ 100: 0.370 | Hits@ 100: 0.604\n",
      "DCG@ 500: 0.401 | Hits@ 500: 0.844\n",
      "DCG@1000: 0.417 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#4: def tknz + lowercase & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей задаче регистр не так важен, как, например, в Named Entity Recognition, что отражается в увеличении значении метрик при использовании lowercase.\n",
    "\n",
    "Увеличение качества на предобученных эмбеддингах связано с тем, что они содержат только слова в lowercase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Python' in so_embeddings = False\n",
      "'python'.lower() in so_embeddings = True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{'Python' in so_embeddings = }\",\n",
    "    f\"{'python'.lower() in so_embeddings = }\",\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(so_embeddings.vocab) = 1787145\n",
      "ascii_uppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
      "There are only lowercase words in the stackoverflow embeddings vocab\n"
     ]
    }
   ],
   "source": [
    "from string import ascii_uppercase\n",
    "\n",
    "print(f\"{len(so_embeddings.vocab) = }\")\n",
    "print(f\"{ascii_uppercase = }\")\n",
    "for word in so_embeddings.vocab:\n",
    "    if any(ch in word for ch in ascii_uppercase):\n",
    "        print(\"Example of an non-lowercase word in the stackoverflow embeddings vocab:\", word)\n",
    "        break\n",
    "else:\n",
    "    print(\"There are only lowercase words in the stackoverflow embeddings vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackoverflow embeddings vocab examples:\n",
      "['use', 'diplayed', 'udpated', 'phpcr', '75x', 'unviewable', 'com/it-it/library/system', 'not-recommended', 'another_user', 'regusers', 'com/components/#dropdowns', 'rmiinterface', 'updatemask', '50+50', 'onfileopen', 'tophone', '9193', 'appusermodelids', 'ÑÐºÐ', '_levels', 'programmer/user', 'greenrectangle', 'intar', 'casescontroller', 'com/docs/security/', 'trasnaction', '/app-root/runtime/repo', 'nav-direction', 'jaredc', 'start+duration', 'com/en-us/sql/t-sql/language-elements/case-transact-sql', 'e103', 'anyway/', 'staa', 'limitedness', 'draw_size', 'com/border-image', 'art_sifra', 'xmlaolap4jutil', 'uibuttion', 'domagoj', 'check/apply', '10994058', 'make-account', 'yourmode', 'type/record', '823x776', 'somethod', 'num_order', '9/20/2014', 'filter14']\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "\n",
    "how_many_words_to_show = 50\n",
    "\n",
    "soe_vocab_examples = list(islice(so_embeddings.vocab.keys(), 0, len(so_embeddings.vocab), int(len(so_embeddings.vocab) / how_many_words_to_show)))\n",
    "print(\"stackoverflow embeddings vocab examples:\", soe_vocab_examples, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Увеличества качества с нашими эмбеддингами вероятно связано с тем, что при обучении они чаще встречают слова с одинаковым смыслом, которые в разном регистре воспринимались бы отдельными сущностями, что должно сильно влиять на их обучение, т.к. корпус мал, особенно в сравнении с целым stackoverflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #5:\n",
    "word_tokenize from nltk + lowercase & stackoverflow embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess(data):\n",
    "    return word_tokenize(data.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223e978a3650478199821483cf269c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744f3dcab91845a19dbd37bd1f0c7f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.399 | Hits@   1: 0.399\n",
      "DCG@   5: 0.487 | Hits@   5: 0.566\n",
      "DCG@  10: 0.508 | Hits@  10: 0.633\n",
      "DCG@ 100: 0.554 | Hits@ 100: 0.858\n",
      "DCG@ 500: 0.569 | Hits@ 500: 0.969\n",
      "DCG@1000: 0.573 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#5: word_tokenize from nltk + lowercase & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #6:\n",
    "word_tokenize from nltk + lowercase & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_6_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679e180d9f4443ee856d4937bbc8654e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_6_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afe696717624cb7b73294c1ce23397c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.229 | Hits@   1: 0.229\n",
      "DCG@   5: 0.282 | Hits@   5: 0.330\n",
      "DCG@  10: 0.298 | Hits@  10: 0.378\n",
      "DCG@ 100: 0.336 | Hits@ 100: 0.571\n",
      "DCG@ 500: 0.370 | Hits@ 500: 0.838\n",
      "DCG@1000: 0.387 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#6: word_tokenize from nltk + lowercase & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество ухудшилось, посмотрим на примеры токенизации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['converting string to list',\n",
       "  'Convert Google results object (pure js) to Python object'],\n",
       " ['Which HTML 5 Canvas Javascript to use for making an interactive drawing tool?',\n",
       "  'Event handling for geometries in Three.js?'],\n",
       " ['Sending array via Ajax fails',\n",
       "  'Getting all list items of an unordered list in PHP']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = train_data[:3]\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default tokenizer:\n",
      "[['converting', 'string', 'to', 'list', 'Convert', 'Google', 'results', 'object', 'pure', 'js', 'to', 'Python', 'object'], ['Which', 'HTML', '5', 'Canvas', 'Javascript', 'to', 'use', 'for', 'making', 'an', 'interactive', 'drawing', 'tool', 'Event', 'handling', 'for', 'geometries', 'in', 'Three', 'js'], ['Sending', 'array', 'via', 'Ajax', 'fails', 'Getting', 'all', 'list', 'items', 'of', 'an', 'unordered', 'list', 'in', 'PHP']]\n",
      "\n",
      "nltk tokenizer:\n",
      "[['converting', 'string', 'to', 'list', 'Convert', 'Google', 'results', 'object', '(', 'pure', 'js', ')', 'to', 'Python', 'object'], ['Which', 'HTML', '5', 'Canvas', 'Javascript', 'to', 'use', 'for', 'making', 'an', 'interactive', 'drawing', 'tool', '?', 'Event', 'handling', 'for', 'geometries', 'in', 'Three.js', '?'], ['Sending', 'array', 'via', 'Ajax', 'fails', 'Getting', 'all', 'list', 'items', 'of', 'an', 'unordered', 'list', 'in', 'PHP']]\n"
     ]
    }
   ],
   "source": [
    "print(\"default tokenizer:\", list(map(lambda x: tokenizer(' '.join(x)), samples)), sep='\\n', end='\\n\\n')\n",
    "print(\"nltk tokenizer:\", list(map(lambda x: word_tokenize(' '.join(x)), samples)), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизатор из `nltk` не отсеивает пунктуацию. Конечно, он и не должен. На деле наш токенизатор не является таковым, объединяя несколько шагов предобработки.\n",
    "\n",
    "\n",
    "Ухудшение метрик при использовании `nltk`-токенизатора в случае обучения собственных эмбеддингов может быть связано с уменьшением количества слов попадающих в одно окно: между словам остаются токены пунктуации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default tokenizer:\n",
      "['There', 'are', 'a', 'lot', 'of', 'popular', 'reliable', 'languages', 'e', 'g', 'C', 'C', 'Python', 'Which', 'should', 'I', 'learn', 'as', 'the', '1st']\n",
      "\n",
      "nltk tokenizer:\n",
      "['There', 'are', 'a', 'lot', 'of', 'popular', '&', 'reliable', 'languages', ',', 'e.g', '.', 'C++', ',', 'C', '#', ',', 'Python', '.', 'Which', 'should', 'I', 'learn', 'as', 'the', '1st', '?']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"There are a lot of popular & reliable languages, e.g. C++, C#, Python. Which should I learn as the 1st?\"\n",
    "print(\"default tokenizer:\", tokenizer(sentence), sep='\\n', end='\\n\\n')\n",
    "print(\"nltk tokenizer:\", word_tokenize(sentence), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наш токенизатор, однако, не ловит специфичные слова, содержащие небуквенные символы: С++, C#, Three.js.\n",
    "Впрочем, `nltk`-токенизатор тоже не определяет некоторые из них, например, C#. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import printable, ascii_letters, digits, punctuation, whitespace\n",
    "\n",
    "set(printable) == set(ascii_letters + digits + punctuation + whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~\n",
      "' ' '\\t' '\\n' '\\r' '\\x0b' '\\x0c'\n"
     ]
    }
   ],
   "source": [
    "print(*list(punctuation))\n",
    "print(*[repr(ch) for ch in whitespace])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of words with some special chars within by word_tokenize from nltk:\n",
      "\n",
      "punctuation\n",
      "-----------\n",
      "\n",
      "generated words: ['!before', '\"before', '#before', '$before', '%before', '&before', \"'before\", '(before', ')before', '*before', '+before', ',before', '-before', '.before', '/before', ':before', ';before', '<before', '=before', '>before', '?before', '@before', '[before', '\\\\before', ']before', '^before', '_before', '`before', '{before', '|before', '}before', '~before', 'with!in', 'with\"in', 'with#in', 'with$in', 'with%in', 'with&in', \"with'in\", 'with(in', 'with)in', 'with*in', 'with+in', 'with,in', 'with-in', 'with.in', 'with/in', 'with:in', 'with;in', 'with<in', 'with=in', 'with>in', 'with?in', 'with@in', 'with[in', 'with\\\\in', 'with]in', 'with^in', 'with_in', 'with`in', 'with{in', 'with|in', 'with}in', 'with~in', 'after!', 'after\"', 'after#', 'after$', 'after%', 'after&', \"after'\", 'after(', 'after)', 'after*', 'after+', 'after,', 'after-', 'after.', 'after/', 'after:', 'after;', 'after<', 'after=', 'after>', 'after?', 'after@', 'after[', 'after\\\\', 'after]', 'after^', 'after_', 'after`', 'after{', 'after|', 'after}', 'after~']\n",
      "\n",
      "---\n",
      "\n",
      "single token: [[\"'before\"], ['+before'], ['-before'], ['.before'], ['/before'], ['=before'], ['\\\\before'], ['^before'], ['_before'], ['|before'], ['~before'], [\"with'in\"], ['with+in'], ['with-in'], ['with.in'], ['with/in'], ['with=in'], ['with\\\\in'], ['with^in'], ['with_in'], ['with|in'], ['with~in'], ['after+'], ['after-'], ['after/'], ['after='], ['after\\\\'], ['after^'], ['after_'], ['after|'], ['after~']]\n",
      "\n",
      "---\n",
      "\n",
      "splitted: [['!', 'before'], ['``', 'before'], ['#', 'before'], ['$', 'before'], ['%', 'before'], ['&', 'before'], ['(', 'before'], [')', 'before'], ['*', 'before'], [',', 'before'], [':', 'before'], [';', 'before'], ['<', 'before'], ['>', 'before'], ['?', 'before'], ['@', 'before'], ['[', 'before'], [']', 'before'], ['`', 'before'], ['{', 'before'], ['}', 'before'], ['with', '!', 'in'], ['with', \"''\", 'in'], ['with', '#', 'in'], ['with', '$', 'in'], ['with', '%', 'in'], ['with', '&', 'in'], ['with', '(', 'in'], ['with', ')', 'in'], ['with', '*', 'in'], ['with', ',', 'in'], ['with', ':', 'in'], ['with', ';', 'in'], ['with', '<', 'in'], ['with', '>', 'in'], ['with', '?', 'in'], ['with', '@', 'in'], ['with', '[', 'in'], ['with', ']', 'in'], ['with', '`', 'in'], ['with', '{', 'in'], ['with', '}', 'in'], ['after', '!'], ['after', \"''\"], ['after', '#'], ['after', '$'], ['after', '%'], ['after', '&'], ['after', \"'\"], ['after', '('], ['after', ')'], ['after', '*'], ['after', ','], ['after', '.'], ['after', ':'], ['after', ';'], ['after', '<'], ['after', '>'], ['after', '?'], ['after', '@'], ['after', '['], ['after', ']'], ['after', '`'], ['after', '{'], ['after', '}']]\n",
      "\n",
      "whitespace\n",
      "----------\n",
      "\n",
      "generated words: [' before', '\\tbefore', '\\nbefore', '\\rbefore', '\\x0bbefore', '\\x0cbefore', 'with in', 'with\\tin', 'with\\nin', 'with\\rin', 'with\\x0bin', 'with\\x0cin', 'after ', 'after\\t', 'after\\n', 'after\\r', 'after\\x0b', 'after\\x0c']\n",
      "\n",
      "---\n",
      "\n",
      "single token: [['before'], ['before'], ['before'], ['before'], ['before'], ['before'], ['after'], ['after'], ['after'], ['after'], ['after'], ['after']]\n",
      "\n",
      "---\n",
      "\n",
      "splitted: [['with', 'in'], ['with', 'in'], ['with', 'in'], ['with', 'in'], ['with', 'in'], ['with', 'in']]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenization of words with some special chars within by word_tokenize from nltk:\", end='\\n\\n')\n",
    "for char_set, char_desc in [(punctuation, 'punctuation'), (whitespace, 'whitespace')]:\n",
    "    print(char_desc, '-' * len(char_desc), sep='\\n', end='\\n\\n')\n",
    "    generated_words = [ch + 'before' for ch in char_set]\n",
    "    generated_words += ['with' + ch + 'in' for ch in char_set]\n",
    "    generated_words += ['after' + ch for ch in char_set]\n",
    "    tokenized = [word_tokenize(w) for w in generated_words]\n",
    "    print(\"generated words:\", generated_words, end='\\n\\n---\\n\\n')\n",
    "    print(\"single token:\", [tokens for tokens in tokenized if len(tokens) == 1], end='\\n\\n---\\n\\n')\n",
    "    print(\"splitted:\", [tokens for tokens in tokenized if len(tokens) > 1], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stackoverflow embeddings vocab examples:\n",
      "['use', 'diplayed', 'udpated', 'phpcr', '75x', 'unviewable', 'com/it-it/library/system', 'not-recommended', 'another_user', 'regusers', 'com/components/#dropdowns', 'rmiinterface', 'updatemask', '50+50', 'onfileopen', 'tophone', '9193', 'appusermodelids', 'ÑÐºÐ', '_levels', 'programmer/user', 'greenrectangle', 'intar', 'casescontroller', 'com/docs/security/', 'trasnaction', '/app-root/runtime/repo', 'nav-direction', 'jaredc', 'start+duration', 'com/en-us/sql/t-sql/language-elements/case-transact-sql', 'e103', 'anyway/', 'staa', 'limitedness', 'draw_size', 'com/border-image', 'art_sifra', 'xmlaolap4jutil', 'uibuttion', 'domagoj', 'check/apply', '10994058', 'make-account', 'yourmode', 'type/record', '823x776', 'somethod', 'num_order', '9/20/2014', 'filter14']\n",
      "\n",
      "their tokenization by nltk (only which splitted):\n",
      "[['com/components/', '#', 'dropdowns']]\n"
     ]
    }
   ],
   "source": [
    "print(\"stackoverflow embeddings vocab examples:\", soe_vocab_examples, sep='\\n', end='\\n\\n')\n",
    "tokenized_by_nltk = [word_tokenize(w) for w in soe_vocab_examples]\n",
    "print(\"their tokenization by nltk (only which splitted):\", [tokens for tokens in tokenized_by_nltk if len(tokens) > 1], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '..', 'world', '...', 'more', 'dots', '......', 'etc.', ',', 'e.g', '.', 'etc', '.', ':', 'aaaa', '!', '!', '!', 'bbb', '!', '!', 'cc', '?', '?', 'C', '#', 'C/C++']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(\"hello.. world... more dots...... etc., e.g. etc.: aaaa!!! bbb!! cc?? C# C/C++\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation chars as words in the stackoverflow embeddings vocab: ['#', '+', '-', '/', '\\\\', '_']\n",
      "Whitespace chars as words in the stackoverflow embeddings vocab: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Punctuation chars as words in the stackoverflow embeddings vocab:\", [ch for ch in punctuation if ch in so_embeddings])\n",
    "print(\"Whitespace chars as words in the stackoverflow embeddings vocab:\", [ch for ch in whitespace if ch in so_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом токены спецсимволов мы не отсеиваем и некоторые из них присутствуют в словаре предобученных эмбеддингов, что, вероятно, дает некоторый \"шум\", с чем и может быть связано ухудшение на предобученных эмбеддингах. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'C#' in so_embeddings = False\n",
      "'C#'.lower() in so_embeddings = True\n",
      "'C#'.lower() in so_embeddings = True\n",
      "'Three.js' in so_embeddings = False\n",
      "'Three.js'.lower() in so_embeddings = False\n",
      "'.js' in so_embeddings = False\n",
      "'js' in so_embeddings = True\n",
      "'.json' in so_embeddings = False\n",
      "'helloworld' in so_embeddings = True\n",
      "'helloworld.py' in so_embeddings = False\n",
      "'helloworld.cpp' in so_embeddings = False\n",
      "'.cpp' in so_embeddings = False\n",
      "'cpp' in so_embeddings = True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"{'C#' in so_embeddings = }\",\n",
    "    f\"{'C#'.lower() in so_embeddings = }\",\n",
    "    f\"{'C#'.lower() in so_embeddings = }\",\n",
    "    f\"{'Three.js' in so_embeddings = }\",\n",
    "    f\"{'Three.js'.lower() in so_embeddings = }\",\n",
    "    f\"{'.js' in so_embeddings = }\",\n",
    "    f\"{'js' in so_embeddings = }\",\n",
    "    f\"{'.json' in so_embeddings = }\",\n",
    "    f\"{'helloworld' in so_embeddings = }\",\n",
    "    f\"{'helloworld.py' in so_embeddings = }\",\n",
    "    f\"{'helloworld.cpp' in so_embeddings = }\",\n",
    "    f\"{'.cpp' in so_embeddings = }\",\n",
    "    f\"{'cpp' in so_embeddings = }\",\n",
    "    sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no dot within words in the stackoverflow embeddings vocab\n"
     ]
    }
   ],
   "source": [
    "for word in so_embeddings.vocab:\n",
    "    if '.' in word:\n",
    "        print(\"Example of a word with a dot within in the stackoverflow embeddings vocab:\", word)\n",
    "        break\n",
    "else:\n",
    "    print(\"There is no dot within words in the stackoverflow embeddings vocab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Punctuation chars within words in the stackoverflow embeddings vocab: _ \\ + # / -\n"
     ]
    }
   ],
   "source": [
    "soe_chars_within_tokens = set()\n",
    "for word in so_embeddings.vocab:\n",
    "        soe_chars_within_tokens.update(ch for ch in punctuation if ch in word and ch != word)\n",
    "print(\"Punctuation chars within words in the stackoverflow embeddings vocab:\", *soe_chars_within_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, `nltk`-токенизатор определяет как единый токен такие сочетания, как Three.js, helloworld.cpp, но они не присутствуют в предобученных эмбеддингах, т.е. методы токенизации не согласованы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Просматривающиеся пути улучшения качества:\n",
    "- доработка собственного токенизатора добавлением учета спецсимволов в регулярное выражение\n",
    "- добавление постпроцессинга для `nltk`-токенизатора: отсев токенов пунтуации, разбиение токенов наподобие helloworld.cpp, объединение некоторых последовательных токенов, подобных C и #. Последнее проблематично, ибо это обратное преобразование, не являющееся однозначным: спецсимвол мог находится в начале слова, в середине или конце."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #7:\n",
    "improved tokenizer + lowercase & stackoverflow embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "soe_chars_within_tokens = \"-_#/+\\\\\"\n",
    "multi_char_punct_mark_chars = ['.'] \n",
    "word_token_chars = ''.join([re.escape(ch) for ch in soe_chars_within_tokens]) + r\"\\w\"\n",
    "punct_multi_chars = ''.join([re.escape(ch) for ch in multi_char_punct_mark_chars])\n",
    "punct_single_chars = ''.join([re.escape(ch) for ch in punctuation])\n",
    "any_non_whitespace_chars = r\"^\\s\"\n",
    "token_pattern = re.compile(\n",
    "    f\"[{word_token_chars}]+|\"\n",
    "    f\"[{punct_multi_chars}]+|\"\n",
    "    f\"[{punct_single_chars}]|\"\n",
    "    f\"[{any_non_whitespace_chars}]+\"\n",
    "    )\n",
    "\n",
    "\n",
    "def improved_tokenizer(text):\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "\n",
    "def filter_punctuation(tokens, punctuation_chars=punctuation):\n",
    "    for token in tokens:\n",
    "        if any(ch not in punctuation_chars for ch in token):\n",
    "            yield token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"/asdf/jkl C# dir/ Windows\\\\System32 c/c++ .cpp ... ?: e.g. --enable helloworld.py\\tjohn\\ndoe Салтыков-Щедрин\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/asdf/jkl', 'C#', 'dir/', 'Windows\\\\System32', 'c/c++', '.', 'cpp', '...', '?', ':', 'e', '.', 'g', '.', '--enable', 'helloworld', '.', 'py', 'john', 'doe', 'Салтыков-Щедрин']\n"
     ]
    }
   ],
   "source": [
    "print(improved_tokenizer(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return list(filter_punctuation(improved_tokenizer(data.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/asdf/jkl', 'c#', 'dir/', 'windows\\\\system32', 'c/c++', 'cpp', 'e', 'g', '--enable', 'helloworld', 'py', 'john', 'doe', 'салтыков-щедрин']\n"
     ]
    }
   ],
   "source": [
    "print(preprocess(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b0d98d40dc439ea682ee9fc817d4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f7627749b8410d9fb54162723669b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.411 | Hits@   1: 0.411\n",
      "DCG@   5: 0.495 | Hits@   5: 0.573\n",
      "DCG@  10: 0.516 | Hits@  10: 0.638\n",
      "DCG@ 100: 0.561 | Hits@ 100: 0.864\n",
      "DCG@ 500: 0.576 | Hits@ 500: 0.971\n",
      "DCG@1000: 0.579 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#7: improved tokenizer + lowercase & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Немного не дотянули до результатов с простейшим токенизатором + lowercase на предобученных эмбеддингах (эксперимент #3). **Почему?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #8:\n",
    "improved tokenizer + lowercase & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 42.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_8_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb890dfcfa94a1a8a487dca2aacba0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_8_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947aa4f41622454fa11ea9d4887668cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.250 | Hits@   1: 0.250\n",
      "DCG@   5: 0.304 | Hits@   5: 0.351\n",
      "DCG@  10: 0.320 | Hits@  10: 0.402\n",
      "DCG@ 100: 0.358 | Hits@ 100: 0.589\n",
      "DCG@ 500: 0.390 | Hits@ 500: 0.838\n",
      "DCG@1000: 0.407 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#8: improved tokenizer + lowercase & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогично, наблюдается ухудшение качества по сравнению с простейшим токенизатором + lowercase на наших собственных эмбеддингах (эксперимент #4). **Почему?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #9:\n",
    "word_tokenize from nltk + lowercase + postprocess (punctuation filter, split by dot) & stackoverflow embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "SEPARATORS = \".\"\n",
    "split_pattern = re.compile('|'.join(re.escape(ch) for ch in SEPARATORS))\n",
    "\n",
    "\n",
    "def split_words(words_list, separators=SEPARATORS):\n",
    "    result = []\n",
    "    for word in words_list:\n",
    "        result.extend(split_pattern.split(word))\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    return list(filter_punctuation(split_words(word_tokenize(data.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e169e76727f44524abaa57267f4e36fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d678a2b01cf74c2bbdc6d415546d7a75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.408 | Hits@   1: 0.408\n",
      "DCG@   5: 0.493 | Hits@   5: 0.573\n",
      "DCG@  10: 0.514 | Hits@  10: 0.636\n",
      "DCG@ 100: 0.560 | Hits@ 100: 0.862\n",
      "DCG@ 500: 0.574 | Hits@ 500: 0.970\n",
      "DCG@1000: 0.577 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#9: word_tokenize from nltk + lowercase + punct filter + split by dot & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизатор из `nltk` по-прежнему работает хуже самописного (и default (#3), и improved (#7)) на предобученных эмбеддингах, даже вместе отсеиванием пунктуации и разбиением по точке, но по сравнению с его предыдущим результатом (эксперимент #5) результат улучшился."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #10:\n",
    "word_tokenize from nltk + lowercase + postprocess (punctuation filter, split by dot) & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_10_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb890d1f8e941d7ab92e00b699d9cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_10_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b40fd58b1e543a08f39b22b81951a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.254 | Hits@   1: 0.254\n",
      "DCG@   5: 0.304 | Hits@   5: 0.347\n",
      "DCG@  10: 0.323 | Hits@  10: 0.405\n",
      "DCG@ 100: 0.360 | Hits@ 100: 0.592\n",
      "DCG@ 500: 0.391 | Hits@ 500: 0.840\n",
      "DCG@1000: 0.408 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#10: word_tokenize from nltk + lowercase + punct filter + split by dot & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На наших эмбеддингах вместе с отсеиванием пунктуации и разбиением по точке `nltk`-токенизатор улучшил свои результаты (эксперимент #6), а также побил/чуть уступил improved tokenizer из #8 \\[не всегда это получается, зависит от seed], но не дотянулся до default tokenizer (#4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default tokenizer VS improved tokenizer\n",
    "Попробум разобраться в подобных результатах работы улучшенного токенизатора, сравнив его с исходным по рангу дубликатов на валидационных данных. Для начала с использованием предобученных эмбеддингов, при использовании которых с улучшенным токенизатором качество упало с 0.415 (#3) до 0.411 (#7) \\[результат одинаков на паре прогонов]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_tokenizers_using_validation(embeddings, tokenizer1, tokenizer2, validation_data, max_validation_examples=MAX_VAL_EX):\n",
    "    val_data_inds_on_which_diff_rank = []\n",
    "    for i, line in tqdm(\n",
    "            enumerate(islice(validation_data, max_validation_examples)),\n",
    "            total=max_validation_examples):\n",
    "        q, *ex = line\n",
    "        ranks1 = rank_candidates(q, ex, embeddings, tokenizer1)\n",
    "        ranking1 = [r[0] for r in ranks1].index(0) + 1\n",
    "        ranks2 = rank_candidates(q, ex, embeddings, tokenizer2)\n",
    "        ranking2 = [r[0] for r in ranks2].index(0) + 1\n",
    "        if ranking1 != ranking2:\n",
    "            val_data_inds_on_which_diff_rank.append((i, (ranking1, ranking2)))\n",
    "    return val_data_inds_on_which_diff_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_with_default_tokenizer(data):  # expr #3\n",
    "    return tokenizer(data.lower())\n",
    "\n",
    "def preprocess_with_improved_tokenizer(data):  # expr 7 \n",
    "    return list(filter_punctuation(improved_tokenizer(data.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e55432dfab2460b9f541c1309afa34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_data_inds_on_which_diff_rank = compare_tokenizers_using_validation(so_embeddings, preprocess_with_default_tokenizer, preprocess_with_improved_tokenizer, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_data_inds_on_which_diff_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tokenization_result(val_data_inds_on_which_diff_rank, tok1, tok2, tok1_name=\"tok1\", tok2_name=\"tok2\", min_rank_diff=-1, max_q_from_one_val=5):\n",
    "    for val_data_i, (tok1_rank, tok2_rank) in val_data_inds_on_which_diff_rank:\n",
    "        rank_diff = tok1_rank - tok2_rank\n",
    "        if ((min_rank_diff > 0) & (rank_diff >= min_rank_diff)) or ((min_rank_diff < 0) & (rank_diff <= min_rank_diff)):\n",
    "            if rank_diff > 0:\n",
    "                w1, w2 = \"worse\", \"better\"\n",
    "            else:\n",
    "                w1, w2 = \"better\", \"worse\"\n",
    "            q_counter = 0\n",
    "            for i, q in enumerate(validation_data[val_data_i]):\n",
    "                t1_res = tok1(q)\n",
    "                t2_res = tok2(q)\n",
    "                if t1_res != t2_res:\n",
    "                    q_counter += 1\n",
    "                    if q_counter > max_q_from_one_val:\n",
    "                        break \n",
    "                    print(f\"{tok1_name} ({w1} res) & {tok2_name} ({w2} res) on {i} query from val data #{val_data_i}:\", t1_res,t2_res, sep='\\n', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default tokenizer is *better* (rank less) than improved tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ny2']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 21 query from val data #390:\n",
      "['f', 'nativeptr', 'stackalloc', 'unexpected', 'stack', 'overflow']\n",
      "['f#', 'nativeptr', 'stackalloc', 'unexpected', 'stack', 'overflow']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 0 query from val data #395:\n",
      "['android', 'de', 'facto', 'implementation', 'of', 'rest', 'json', 'client']\n",
      "['android', 'de-facto', 'implementation', 'of', 'rest/json', 'client']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 4 query from val data #395:\n",
      "['insert', 'close', 'row', 'after', 'every', '4', 'in', 'foreach', 'loop']\n",
      "['insert/close', 'row', 'after', 'every', '4', 'in', 'foreach', 'loop']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 5 query from val data #395:\n",
      "['use', 'internal', 'static', 'class', 'methods', 'from', 'external', 'dll', 'using', 'c']\n",
      "['use', 'internal', 'static', 'class', 'methods', 'from', 'external', 'dll', 'using', 'c#']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 16 query from val data #395:\n",
      "['jquery', 'masked', 'input', 'format', 'date', 'as', 'm', 'd', 'yyyy', 'or', 'm', 'dd', 'yyyy', 'or', 'mm', 'dd', 'yyyy', 'or', 'mm', 'd', 'yyyy']\n",
      "['jquery', 'masked', 'input', 'format', 'date', 'as', 'm/d/yyyy', 'or', 'm/dd/yyyy', 'or', 'mm/dd/yyyy', 'or', 'mm/d/yyyy']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 22 query from val data #395:\n",
      "['nginx', 'php', 'fpm', 'bad', 'gateway', 'only', 'when', 'xdebug', 'server', 'is', 'running']\n",
      "['nginx', 'php-fpm', 'bad', 'gateway', 'only', 'when', 'xdebug', 'server', 'is', 'running']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 0 query from val data #399:\n",
      "['sorting', 'searching', 'on', 'a', 'table', 'is', 'not', 'working']\n",
      "['sorting/searching', 'on', 'a', 'table', 'is', 'not', 'working']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #399:\n",
      "['sorting', 'filtering', 'mvc']\n",
      "['sorting/filtering', 'mvc']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 3 query from val data #399:\n",
      "['removing', 'qcombobox', 'drop', 'down', 'animation']\n",
      "['removing', 'qcombobox', 'drop-down', 'animation']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 23 query from val data #399:\n",
      "['csrf', 'token', 'session_store', 'with', 'ember', 'simple', 'auth', 'alongside', 'devise']\n",
      "['csrf', 'token', 'session_store', 'with', 'ember-simple-auth', 'alongside', 'devise']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 27 query from val data #399:\n",
      "['c', 'file', 'seekg', 'does', 'not', 'appear', 'to', 'return', 'current', 'location']\n",
      "['c++', 'file', 'seekg', 'does', 'not', 'appear', 'to', 'return', 'current', 'location']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 0 query from val data #403:\n",
      "['hibernate', 'jpa', 'add', 'a', 'calculated', 'field']\n",
      "['hibernate/jpa', 'add', 'a', 'calculated', 'field']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 14 query from val data #403:\n",
      "['curl', 'x', 'post', 'd', 'mapping', 'json', 'mapping', 'not', 'created']\n",
      "['curl', '-x', 'post', '-d', 'mapping', 'json', 'mapping', 'not', 'created']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 16 query from val data #403:\n",
      "['css', 'semi', 'transparent', 'background', 'and', 'an', 'image']\n",
      "['css', 'semi-transparent', 'background', 'and', 'an', 'image']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 25 query from val data #403:\n",
      "['is', 'std', 'make_unique', 'sfinae', 'friendly']\n",
      "['is', 'std', 'make_unique', 'sfinae-friendly']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 32 query from val data #403:\n",
      "['address', 'of', 'array', 'vs', 'pointer', 'to', 'pointer', 'not', 'the', 'same']\n",
      "['address', 'of', 'array', 'vs', 'pointer-to-pointer', 'not', 'the', 'same']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #415:\n",
      "['how', 'to', 'add', 'pinch', 'zoom', 'behavior', 'using', 'jquery', 'mobile']\n",
      "['how', 'to', 'add', 'pinch/zoom', 'behavior', 'using', 'jquery', 'mobile']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 9 query from val data #415:\n",
      "['using', 'ifdef', 'or', 'if', 'defined', 'to', 'find', 'where', 'a', 'variable', 'is', 'not', 'defined']\n",
      "['using', '#ifdef', 'or', '#if', 'defined', 'to', 'find', 'where', 'a', 'variable', 'is', 'not', 'defined']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 18 query from val data #415:\n",
      "['what', 's', 'the', 'difference', 'between', 'table', 'insert', 't', 'i', 'and', 't', 't', '1', 'i']\n",
      "['what', 's', 'the', 'difference', 'between', 'table', 'insert', 't', 'i', 'and', 't', '#t+1', 'i']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 26 query from val data #415:\n",
      "['sql', 'job', 'result', 'in', 'c']\n",
      "['sql', 'job', 'result', 'in', 'c#']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 28 query from val data #415:\n",
      "['c', 'and', 'wpf', 'crisis', 'with', 'blank', 'listbox', 'items']\n",
      "['c#', 'and', 'wpf', 'crisis', 'with', 'blank', 'listbox', 'items']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #437:\n",
      "['datasnap', 'firedac', 'query', 'executed', 'twice']\n",
      "['datasnap\\\\firedac', 'query', 'executed', 'twice']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 30 query from val data #437:\n",
      "['combining', 'flexc', 'and', 'bisonc']\n",
      "['combining', 'flexc++', 'and', 'bisonc++']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 32 query from val data #437:\n",
      "['beid', 'c', 'cryptoacquirefailed', 'error', '80090019']\n",
      "['beid', 'c#', 'cryptoacquirefailed', 'error', '80090019']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 35 query from val data #437:\n",
      "['why', 'ng', 'model', 'resets', 'selected', 'option']\n",
      "['why', 'ng-model', 'resets', 'selected', 'option']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 48 query from val data #437:\n",
      "['resources', 'for', 'drawable', 'xlarge', 'mdpi', 'and', 'drawable', 'sw600dp', 'mdpi', 'without', 'duplication']\n",
      "['resources', 'for', 'drawable-xlarge-mdpi', 'and', 'drawable-sw600dp-mdpi', 'without', 'duplication']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 0 query from val data #495:\n",
      "['can', 'the', 'application', 'be', 'reset', 'programmatically', 'in', 'objective', 'c']\n",
      "['can', 'the', 'application', 'be', 'reset', 'programmatically', 'in', 'objective-c']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 54 query from val data #495:\n",
      "['xml', 'parsing', 'in', 'oracle', 'pl', 'sql']\n",
      "['xml', 'parsing', 'in', 'oracle', 'pl/sql']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 62 query from val data #495:\n",
      "['google', 'freebase', 'api', 'c', 'net', 'example']\n",
      "['google', 'freebase', 'api', 'c#', 'net', 'example']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 67 query from val data #495:\n",
      "['automatic', 'aws', 'dynamodb', 'to', 's3', 'export', 'failing', 'with', 'role', 'datapipelinedefaultrole', 'is', 'invalid']\n",
      "['automatic', 'aws', 'dynamodb', 'to', 's3', 'export', 'failing', 'with', 'role/datapipelinedefaultrole', 'is', 'invalid']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 70 query from val data #495:\n",
      "['does', 'a', 'combined', 'pop', 'top', 'stack', 'already', 'exist', 'in', 'std']\n",
      "['does', 'a', 'combined', 'pop/top', 'stack', 'already', 'exist', 'in', 'std']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #535:\n",
      "['why', 'is', 'the', 'xslt', 'disable', 'output', 'escaping', 'not', 'implemented', 'in', 'firefox']\n",
      "['why', 'is', 'the', 'xslt', 'disable-output-escaping', 'not', 'implemented', 'in', 'firefox']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 8 query from val data #535:\n",
      "['changing', 'color', 'for', 'each', 'card', 'using', 'angularjs', 'ng', 'class']\n",
      "['changing', 'color', 'for', 'each', 'card', 'using', 'angularjs', 'ng-class']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 17 query from val data #535:\n",
      "['jenkins', 'cli', 'exception', 'missing', 'job', 'extendedread', 'permission']\n",
      "['jenkins', 'cli', 'exception', 'missing', 'job/extendedread', 'permission']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 23 query from val data #535:\n",
      "['xcode', '7', 'ui', 'testing', 'dismissal', 'of', 'system', 'generated', 'uialertcontroller', 'does', 'not', 'work']\n",
      "['xcode', '7', 'ui', 'testing', 'dismissal', 'of', 'system-generated', 'uialertcontroller', 'does', 'not', 'work']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 31 query from val data #535:\n",
      "['javascript', 'natural', 'sort', 'array', 'object', 'and', 'maintain', 'index', 'association']\n",
      "['javascript', 'natural', 'sort', 'array/object', 'and', 'maintain', 'index', 'association']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 0 query from val data #556:\n",
      "['how', 'to', 'make', 'json', 'schema', 'to', 'allow', 'one', 'but', 'not', 'another', 'field']\n",
      "['how', 'to', 'make', 'json-schema', 'to', 'allow', 'one', 'but', 'not', 'another', 'field']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 9 query from val data #556:\n",
      "['mvc', 'model', 'binding', 'to', 'a', 'dictionary', 'with', 'list', 'ienumerable', 'as', 'value']\n",
      "['mvc', 'model-binding', 'to', 'a', 'dictionary', 'with', 'list', 'ienumerable', 'as', 'value']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 12 query from val data #556:\n",
      "['internet', 'access', 'in', 'a', 'long', 'running', 'background', 'task']\n",
      "['internet', 'access', 'in', 'a', 'long-running', 'background', 'task']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 18 query from val data #556:\n",
      "['facebook', 'fan', 'box', 'stopped', 'respecting', 'the', 'css', 'attribute']\n",
      "['facebook', 'fan-box', 'stopped', 'respecting', 'the', '“css”', 'attribute']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 36 query from val data #556:\n",
      "['is', 'better', 'load', 'twitter', 'posts', 'client', 'side', 'or', 'server', 'side']\n",
      "['is', 'better', 'load', 'twitter', 'posts', 'client-side', 'or', 'server-side']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #566:\n",
      "['can', 'cq5', 'adobe', 'experience', 'manager', 'be', 'used', 'headless']\n",
      "['can', 'cq5/adobe', 'experience', 'manager', 'be', 'used', 'headless']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 32 query from val data #566:\n",
      "['if', 'else', 'at', 'compile', 'time']\n",
      "['if/else', 'at', 'compile', 'time']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 37 query from val data #566:\n",
      "['de', 'serializing', 'json', 'object', 'with', 'property', 'named', 'return']\n",
      "['de-serializing', 'json', 'object', 'with', 'property', 'named', 'return']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 47 query from val data #566:\n",
      "['ng', 'model', 'is', 'not', 'picking', 'the', 'input', 'type', 'tel', 'value', 'in', 'angularjs']\n",
      "['ng-model', 'is', 'not', 'picking', 'the', 'input', 'type', 'tel', 'value', 'in', 'angularjs']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 48 query from val data #566:\n",
      "['while', 'trying', 'to', 'resolve', 'a', 'cross', 'store', 'reference', 'the', 'sid', 'of', 'the', 'target', 'principal', 'could', 'not', 'be', 'resolved', 'the', 'error', 'code', 'is', '1332']\n",
      "['while', 'trying', 'to', 'resolve', 'a', 'cross-store', 'reference', 'the', 'sid', 'of', 'the', 'target', 'principal', 'could', 'not', 'be', 'resolved', 'the', 'error', 'code', 'is', '1332']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #633:\n",
      "['c', 'wpf', 'dragmove', 'and', 'click']\n",
      "['c#', 'wpf', 'dragmove', 'and', 'click']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 23 query from val data #633:\n",
      "['how', 'do', 'you', 'utilize', 'more', 'than', '9', 'arguments', 'when', 'calling', 'a', 'label', 'in', 'a', 'cmd', 'batch', 'script']\n",
      "['how', 'do', 'you', 'utilize', 'more', 'than', '9', 'arguments', 'when', 'calling', 'a', 'label', 'in', 'a', 'cmd', 'batch-script']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 33 query from val data #633:\n",
      "['how', 'to', 'remove', 'custom', 'post', 'type', 'slug', 'taxonomy', 'base', 'from', 'url', 'in', 'wordpress']\n",
      "['how', 'to', 'remove', 'custom', 'post', 'type', 'slug', 'taxonomy-base', 'from', 'url', 'in', 'wordpress']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 36 query from val data #633:\n",
      "['can', 'you', 'specify', 'which', 'cipher', 'suite', 'your', 'java', 'application', 'client', 'should', 'use']\n",
      "['can', 'you', 'specify', 'which', 'cipher', 'suite', 'your', 'java', 'application/client', 'should', 'use']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 44 query from val data #633:\n",
      "['es6', 'generators', 'example', 'where', 'there', 'is', 'no', 'yield', 'expression', 'for', 'the', 'first', 'next']\n",
      "['es6', 'generators-', 'example', 'where', 'there', 'is', 'no', 'yield', 'expression', 'for', 'the', 'first', 'next']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 0 query from val data #642:\n",
      "['what', 's', 'the', 'best', 'way', 'to', 'share', 'a', 'texture', 'image', 'between', 'two', 'context', 'with', 'out', 'context', 'sharing']\n",
      "['what', 's', 'the', 'best', 'way', 'to', 'share', 'a', 'texture/image', 'between', 'two', 'context', 'with', 'out', 'context', 'sharing']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 10 query from val data #642:\n",
      "['c', 'append', 'data', 'on', 'a', 'txt', 'file', 'in', 'a', 'specific', 'way']\n",
      "['c++', 'append', 'data', 'on', 'a', 'txt', 'file', 'in', 'a', 'specific', 'way']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 14 query from val data #642:\n",
      "['how', 'to', 'verify', 'network', 'connectivity', 'in', 'objective', 'c']\n",
      "['how', 'to', 'verify', 'network', 'connectivity', 'in', 'objective-c']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 20 query from val data #642:\n",
      "['ffmpeg', 'cannot', 'find', 'executebinaryresponsehandler', 'android', 'java']\n",
      "['ffmpeg', 'cannot', 'find', 'executebinaryresponsehandler', 'android/java']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 25 query from val data #642:\n",
      "['how', 'to', 'programmatically', 'construct', 'components', 'at', 'runtime', 'using', 'c', 'builder']\n",
      "['how', 'to', 'programmatically', 'construct', 'components', 'at', 'runtime', 'using', 'c++', 'builder']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #937:\n",
      "['saving', 'and', 'printing', 'information', 'in', 'c', 'net', 'form']\n",
      "['saving', 'and', 'printing', 'information', 'in', 'c#', 'net', 'form']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 12 query from val data #937:\n",
      "['create', 'sharepoint', '2010', 'ribbon', 'button', 'programmatically', 'w', 'o', 'feature', 'xml']\n",
      "['create', 'sharepoint', '2010', 'ribbon', 'button', 'programmatically', 'w/o', 'feature', 'xml']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 30 query from val data #937:\n",
      "['node', 'can', 't', 'start', 'after', 'vue', 'js', 'cli', 'install']\n",
      "['node', 'can', 't', 'start', 'after', 'vue', 'js-cli', 'install']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 31 query from val data #937:\n",
      "['copy', 'paste', 'cut', 'paste', 'in', 'c', 'windows', 'forms', 'datagridview']\n",
      "['copy', 'paste', 'cut', 'paste', 'in', 'c++', 'windows', 'forms', 'datagridview']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 36 query from val data #937:\n",
      "['error', 'attempt', 'to', 'use', 'zero', 'length', 'variable', 'name']\n",
      "['error', 'attempt', 'to', 'use', 'zero-length', 'variable', 'name']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #951:\n",
      "['gsutil', 'cors', 'set', 'origin', 'fails', 'to', 'make', 'access', 'control', 'allow', 'origin', 'to', 'be', 'updated']\n",
      "['gsutil', 'cors', 'set', 'origin', 'fails', 'to', 'make', 'access-control-allow-origin', 'to', 'be', 'updated']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 8 query from val data #951:\n",
      "['paypal', 'ipn', 'returning', 'http', '1', '1', '200', 'ok']\n",
      "['paypal', 'ipn', 'returning', 'http/1', '1', '200', 'ok']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 10 query from val data #951:\n",
      "['react', 'native', 'when', 'enable', 'proguard', 'the', 'app', 'close', 'and', 'not', 'run']\n",
      "['react-native', 'when', 'enable', 'proguard', 'the', 'app', 'close', 'and', 'not', 'run']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 14 query from val data #951:\n",
      "['unable', 'to', 'run', 'aapt', 'command', 'keeps', 'returning', 'android', 'sdk', 'build', 'tools']\n",
      "['unable', 'to', 'run', 'aapt', 'command', 'keeps', 'returning', 'android', 'sdk', 'build-tools']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 26 query from val data #951:\n",
      "['unity3d', 'audio', 'playing', 'getting', 'muted', 'after', 'getting', 'any', 'call', 'notification', 'on', 'android']\n",
      "['unity3d', 'audio', 'playing', 'getting', 'muted', 'after', 'getting', 'any', 'call/notification', 'on', 'android']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #967:\n",
      "['sprite', 'kit', 'how', 'to', 'visualize', 'an', 'skfieldnode']\n",
      "['sprite-kit', 'how', 'to', 'visualize', 'an', 'skfieldnode']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 7 query from val data #967:\n",
      "['does', 'a', 'combined', 'pop', 'top', 'stack', 'already', 'exist', 'in', 'std']\n",
      "['does', 'a', 'combined', 'pop/top', 'stack', 'already', 'exist', 'in', 'std']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 11 query from val data #967:\n",
      "['c', 'merge', 'multiple', 'lists', 'based', 'on', 'timestamp']\n",
      "['c#', 'merge', 'multiple', 'lists', 'based', 'on', 'timestamp']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 12 query from val data #967:\n",
      "['mysql', 'match', 'against', 'when', 'searching', 'e', 'mail', 'addresses']\n",
      "['mysql', 'match', 'against', 'when', 'searching', 'e-mail', 'addresses']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 17 query from val data #967:\n",
      "['my', 'body', 'background', 'image', 'does', 'not', 'showing', 'up', 'for', 'the', 'entire', 'page']\n",
      "['my', 'body', 'background-image', 'does', 'not', 'showing', 'up', 'for', 'the', 'entire', 'page']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 1 query from val data #989:\n",
      "['javax', 'mail', 'messagingexception', 'in', 'jar', 'file', 'but', 'same', 'code', 'working', 'in', 'eclipse']\n",
      "['javax/mail/messagingexception', 'in', 'jar', 'file', 'but', 'same', 'code', 'working', 'in', 'eclipse']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 26 query from val data #989:\n",
      "['facing', 'difficulty', 'in', 'constructing', 'sql', 'from', 'multiple', 'multi', 'select', 'listboxes']\n",
      "['facing', 'difficulty', 'in', 'constructing', 'sql', 'from', 'multiple', 'multi-select', 'listboxes']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 28 query from val data #989:\n",
      "['canvas', 'like', 'element']\n",
      "['canvas-like', 'element']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 36 query from val data #989:\n",
      "['php', 'if', 'pagerank', 'is', 'less', 'than', '0', 'output', 'n', 'a']\n",
      "['php', 'if', 'pagerank', 'is', 'less', 'than', '0', 'output', 'n/a']\n",
      "\n",
      "default tokenizer (better res) & improved tokenizer (worse res) on 42 query from val data #989:\n",
      "['when', 'to', 'use', 'non', 'comparison', 'sorting', 'over', 'comparison', 'sorting']\n",
      "['when', 'to', 'use', 'non-comparison', 'sorting', 'over', 'comparison', 'sorting']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MIN_RANK_DIFF = -50\n",
    "MAX_QUERIES_FROM_ONE_VAL_SAMPLE = 5\n",
    "show_tokenization_result(val_data_inds_on_which_diff_rank, preprocess_with_default_tokenizer, preprocess_with_improved_tokenizer, \"default tokenizer\", \"improved tokenizer\", min_rank_diff=MIN_RANK_DIFF, max_q_from_one_val=MAX_QUERIES_FROM_ONE_VAL_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### default tokenizer is *worse* (rank higher) than improved tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #12:\n",
      "['hierarchy', 'parent', 'child', 'relationship', 'datagrid', 'c', 'window', 'application']\n",
      "['hierarchy', 'parent', 'child', 'relationship', 'datagrid', 'c#', 'window', 'application']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 1 query from val data #12:\n",
      "['expanding', 'collapsing', 'in', 'datagridview']\n",
      "['expanding/collapsing', 'in', 'datagridview']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 19 query from val data #12:\n",
      "['unable', 'to', 'run', 'rails', 'server', 'require', 'cannot', 'load', 'such', 'file', 'net', 'ssh', 'loaderror']\n",
      "['unable', 'to', 'run', 'rails', 'server', 'require', 'cannot', 'load', 'such', 'file', 'net/ssh', 'loaderror']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 21 query from val data #12:\n",
      "['how', 'to', 'error', 'handle', 'overclicking', 'in', 'c']\n",
      "['how', 'to', 'error', 'handle', 'overclicking', 'in', 'c#']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 22 query from val data #12:\n",
      "['prettyphoto', 'social', 'tools', 'do', 'not', 'share', 'lightbox', 'deep', 'link']\n",
      "['prettyphoto', 'social', 'tools', 'do', 'not', 'share', 'lightbox', 'deep-link']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #235:\n",
      "['httpwebrequest', 'to', 'web', 'service', 'in', 'c', 'how', 'to', 'get', 'data', 'fast', 'from', 'the', 'response', 'stream']\n",
      "['httpwebrequest', 'to', 'web-service', 'in', 'c#', 'how', 'to', 'get', 'data', 'fast', 'from', 'the', 'response', 'stream']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 11 query from val data #235:\n",
      "['how', 'to', 'prepare', 'a', 'c', 'string', 'for', 'sql', 'query']\n",
      "['how', 'to', 'prepare', 'a', 'c++', 'string', 'for', 'sql', 'query']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 53 query from val data #235:\n",
      "['does', 'android', 's', 'fileprovider', 'actually', 'support', 'external', 'files', 'path']\n",
      "['does', 'android', 's', 'fileprovider', 'actually', 'support', 'external-files-path']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 74 query from val data #235:\n",
      "['gulp', 'zip', 'with', 'parent', 'folder']\n",
      "['gulp-zip', 'with', 'parent', 'folder']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 75 query from val data #235:\n",
      "['list', 'of', 'variables', 'xml', 'in', 'combo', 'box', 'c']\n",
      "['list', 'of', 'variables', 'xml', 'in', 'combo', 'box', 'c#']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #236:\n",
      "['hiveserver2', 'failed', 'to', 'create', 'change', 'scratchdir', 'permissions', 'to', '777', 'could', 'not', 'create', 'fileclient']\n",
      "['hiveserver2', 'failed', 'to', 'create/change', 'scratchdir', 'permissions', 'to', '777', 'could', 'not', 'create', 'fileclient']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 1 query from val data #236:\n",
      "['impala', 'shell', 'query', 'failing', 'with', 'error', '13']\n",
      "['impala-shell', 'query', 'failing', 'with', 'error', '13']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 9 query from val data #236:\n",
      "['wpf', 'two', 'way', 'binding', 'xml']\n",
      "['wpf', 'two-way', 'binding', 'xml']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 13 query from val data #236:\n",
      "['htmlrenderreport', 'of', 'birt', 'adds', 'a', 'file', 'like', 'url', 'to', 'embedded', 'images', 'instead', 'of', 'embedding', 'them', 'into', 'the', 'html']\n",
      "['htmlrenderreport', 'of', 'birt', 'adds', 'a', 'file', '-like', 'url', 'to', 'embedded', 'images', 'instead', 'of', 'embedding', 'them', 'into', 'the', 'html']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 18 query from val data #236:\n",
      "['why', 'does', 'net', 's', 'conditional', 'attribute', 'cause', 'side', 'effects', 'to', 'be', 'removed']\n",
      "['why', 'does', 'net', 's', 'conditional', 'attribute', 'cause', 'side-effects', 'to', 'be', 'removed']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 1 query from val data #352:\n",
      "['aligning', 'labels', 'in', 'array', 'in', 'c', 'to', 'form', 'a', 'grid']\n",
      "['aligning', 'labels', 'in', 'array', 'in', 'c#', 'to', 'form', 'a', 'grid']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 21 query from val data #352:\n",
      "['delay', 'loading', 'testcasesource', 'in', 'nunit']\n",
      "['delay-loading', 'testcasesource', 'in', 'nunit']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 25 query from val data #352:\n",
      "['edge', 'ignores', 'script', 'scr', 'in', 'content', 'security', 'policy']\n",
      "['edge', 'ignores', 'script-scr', 'in', 'content', 'security', 'policy']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 26 query from val data #352:\n",
      "['ios', 'correctly', 'adopting', 'a', 'view', 'into', 'an', 'already', 'visible', 'parent']\n",
      "['ios', 'correctly', 'adopting', 'a', 'view', 'into', 'an', 'already-visible', 'parent']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 39 query from val data #352:\n",
      "['i', 'want', 'to', 'select', 'specific', 'row', 'when', 'i', 'click', 'button', 'using', 'php', 'mysql']\n",
      "['i', 'want', 'to', 'select', 'specific', 'row', 'when', 'i', 'click', 'button', 'using', 'php/mysql']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #513:\n",
      "['how', 'to', 'use', 'let', 'to', 'setup', 'local', 'binding', 'from', 'list', 'of', 'two', 'element', 'lists', 'in', 'scheme']\n",
      "['how', 'to', 'use', 'let', 'to', 'setup', 'local', 'binding', 'from', 'list', 'of', 'two-element', 'lists', 'in', 'scheme']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 7 query from val data #513:\n",
      "['passing', 'parameters', 'to', 'php', 'include', 'require', 'construct']\n",
      "['passing', 'parameters', 'to', 'php', 'include/require', 'construct']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 14 query from val data #513:\n",
      "['javafx', '2', '2', 'generated', 'image', 'not', 'shown', 'after', 'building', 'jar', 'file']\n",
      "['javafx', '2', '2', 'generated', 'image', 'not', 'shown', 'after', 'building', 'jar-file']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 15 query from val data #513:\n",
      "['wrapping', 'c', 'functions', 'with', 'gnu', 'linker']\n",
      "['wrapping', 'c++', 'functions', 'with', 'gnu', 'linker']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 16 query from val data #513:\n",
      "['working', 'with', 'hash', 'links']\n",
      "['working', 'with', '#hash', 'links']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #514:\n",
      "['uiviewcontroller', 'ignore', 'left', 'right', 'slide', 'movement']\n",
      "['uiviewcontroller', 'ignore', 'left/right', 'slide', 'movement']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 2 query from val data #514:\n",
      "['angularjs', 'nvd3', 'directives', 'setting', 'color', 'in', 'legend', 'as', 'well', 'as', 'in', 'chart', 'elements']\n",
      "['angularjs-nvd3-directives', 'setting', 'color', 'in', 'legend', 'as', 'well', 'as', 'in', 'chart', 'elements']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 3 query from val data #514:\n",
      "['python', 'tkinter', 'scrollbar', 'graph', 'scale', 'slowing', 'down', 'scrolling']\n",
      "['python', 'tkinter', 'scrollbar/graph', 'scale', 'slowing', 'down', 'scrolling']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 17 query from val data #514:\n",
      "['how', 'to', 'make', 'the', 'number', 'from', 'an', 'array', 'json_string', 'negative']\n",
      "['how', 'to', 'make', 'the', 'number', 'from', 'an', 'array/', 'json_string', 'negative']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 22 query from val data #514:\n",
      "['sudoku', 'solving', 'algorithm', 'with', 'back', 'tracking']\n",
      "['sudoku', 'solving', 'algorithm', 'with', 'back-tracking']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #579:\n",
      "['objective', 'c', 'memory', 'management', 'problem']\n",
      "['objective-c', 'memory', 'management', 'problem']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 14 query from val data #579:\n",
      "['inno', 'setup', 'not', 'creating', 'registry', 'entry', 'in', 'software', 'microsoft', 'windows', 'currentversion', 'uninstall']\n",
      "['inno', 'setup', 'not', 'creating', 'registry', 'entry', 'in', 'software/microsoft/windows/currentversion/uninstall']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 16 query from val data #579:\n",
      "['oracle', 'pl', 'sql', 'table', 'function', 'error']\n",
      "['oracle', 'pl/sql', 'table', 'function', 'error']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 20 query from val data #579:\n",
      "['adding', 'data', 'in', 'the', 'database', 'using', 'microsoft', 'access', 'and', 'oledb', 'in', 'c']\n",
      "['adding', 'data', 'in', 'the', 'database', 'using', 'microsoft', 'access', 'and', 'oledb', 'in', 'c#']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 23 query from val data #579:\n",
      "['how', 'to', 'make', 'sure', 'solr', 'lucene', 'won', 't', 'die', 'with', 'java', 'lang', 'outofmemoryerror']\n",
      "['how', 'to', 'make', 'sure', 'solr/lucene', 'won', 't', 'die', 'with', 'java', 'lang', 'outofmemoryerror']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #684:\n",
      "['adding', 'google', 'play', 'services', 'crashes', 'aapt', 'exe']\n",
      "['adding', 'google-play-services', 'crashes', 'aapt', 'exe']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 2 query from val data #684:\n",
      "['wmi', 'defrag', 'method', 'not', 'found', 'in', 'c']\n",
      "['wmi', 'defrag', 'method', 'not', 'found', 'in', 'c#']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 8 query from val data #684:\n",
      "['how', 'to', 'pass', 'scope', 'variable', 'data', 'from', 'one', 'state', 'to', 'another', 'using', 'angular', 'ui', 'router']\n",
      "['how', 'to', 'pass', 'scope', 'variable', 'data', 'from', 'one', 'state', 'to', 'another', 'using', 'angular', 'ui-router']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 13 query from val data #684:\n",
      "['c', 'oledb', 'connection', 'string', 'issue']\n",
      "['c#', 'oledb', 'connection', 'string', 'issue']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 19 query from val data #684:\n",
      "['angular', '2', 'pipe', 'that', 'transforms', 'json', 'object', 'to', 'pretty', 'printed', 'json']\n",
      "['angular', '2', 'pipe', 'that', 'transforms', 'json', 'object', 'to', 'pretty-printed', 'json']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 1 query from val data #761:\n",
      "['checking', 'pixel', 'color', 'allegro', '5', 'c']\n",
      "['checking', 'pixel', 'color', 'allegro', '5', 'c++']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 4 query from val data #761:\n",
      "['how', 'to', 'calculate', 'the', 'dimensions', 'of', 'the', 'monitor', 'in', 'pyqt4', 'windows', 'linux']\n",
      "['how', 'to', 'calculate', 'the', 'dimensions', 'of', 'the', 'monitor', 'in', 'pyqt4/windows/linux']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 9 query from val data #761:\n",
      "['httpwebrequest', 'to', 'web', 'service', 'in', 'c', 'how', 'to', 'get', 'data', 'fast', 'from', 'the', 'response', 'stream']\n",
      "['httpwebrequest', 'to', 'web-service', 'in', 'c#', 'how', 'to', 'get', 'data', 'fast', 'from', 'the', 'response', 'stream']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 31 query from val data #761:\n",
      "['jquery', 'ui', 'dialog', 'loses', 'pre', 'loaded', 'textarea', 'values']\n",
      "['jquery', 'ui', 'dialog', 'loses', 'pre-loaded', 'textarea', 'values']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 34 query from val data #761:\n",
      "['blocks', 'in', 'objective', 'c', 'parse', 'query']\n",
      "['blocks', 'in', 'objective', 'c', '/parse', 'query']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 0 query from val data #965:\n",
      "['class', 'vs', 'import', 'in', 'header', 'compile', 'time', 'saving', 'with', 'clang']\n",
      "['class', 'vs', '#import', 'in', 'header', 'compile', 'time', 'saving', 'with', 'clang']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 10 query from val data #965:\n",
      "['c', 'memory', 'type', 'bigger', 'than', 'ulong']\n",
      "['c#', 'memory', 'type', 'bigger', 'than', 'ulong']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 17 query from val data #965:\n",
      "['how', 'to', 'debug', 'javascript', 'in', 'nashhorn', 'jdk', '8', 'in', 'netbeans']\n",
      "['how', 'to', 'debug', 'javascript', 'in', 'nashhorn', 'jdk', '8+', 'in', 'netbeans']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 21 query from val data #965:\n",
      "['date', 'in', 'textbox', 'in', 'dd', 'mm', 'yyyy', 'format']\n",
      "['date', 'in', 'textbox', 'in', 'dd/mm/yyyy', 'format']\n",
      "\n",
      "default tokenizer (worse res) & improved tokenizer (better res) on 23 query from val data #965:\n",
      "['oracle', 'select', 'into', 'variable', 'error', 'ora', '00947', 'not', 'enough', 'values']\n",
      "['oracle', 'select', 'into', 'variable', 'error', 'ora-00947', 'not', 'enough', 'values']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MIN_RANK_DIFF = 50\n",
    "show_tokenization_result(val_data_inds_on_which_diff_rank, preprocess_with_default_tokenizer, preprocess_with_improved_tokenizer, \"default tokenizer\", \"improved tokenizer\", min_rank_diff=MIN_RANK_DIFF, max_q_from_one_val=MAX_QUERIES_FROM_ONE_VAL_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Между случаями, когда один лучше другого, какого-то значимого различия не наблюдается, но можно попробовать бить слова по forward slash и hyphen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #11:\n",
    "improved tokenizer v2 (without forward slash & hyphen) + lowercase & stackoverflow embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "soe_chars_within_tokens_without_forward_slash_and_hyphen = \"_#+\\\\\"\n",
    "word_token_chars_v2 = ''.join([re.escape(ch) for ch in soe_chars_within_tokens_without_forward_slash_and_hyphen]) + r\"\\w\"\n",
    "token_pattern_v2 = re.compile(\n",
    "    f\"[{word_token_chars_v2}]+|\"\n",
    "    f\"[{punct_multi_chars}]+|\"\n",
    "    f\"[{punct_single_chars}]|\"\n",
    "    f\"[{any_non_whitespace_chars}]+\"\n",
    "    )\n",
    "\n",
    "\n",
    "def improved_tokenizer_v2(text):\n",
    "    return token_pattern_v2.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return list(filter_punctuation(improved_tokenizer_v2(data.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5e7c94df7c4a05aab814a355bacce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfb4151c29f4d7aa3c0e46d0a614cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.418 | Hits@   1: 0.418\n",
      "DCG@   5: 0.504 | Hits@   5: 0.584\n",
      "DCG@  10: 0.525 | Hits@  10: 0.650\n",
      "DCG@ 100: 0.571 | Hits@ 100: 0.877\n",
      "DCG@ 500: 0.584 | Hits@ 500: 0.974\n",
      "DCG@1000: 0.587 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#11: imprvd tok v2 (wo forward slash & hyphen) + lowercase & & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #12:\n",
    "improved tokenizer v2 (without forward slash & hyphen) + lowercase & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_12_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88c1a0ab5b5c4527a18d12306f892d09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_12_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a6a0f80558471fa194a8853d10526b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.261 | Hits@   1: 0.261\n",
      "DCG@   5: 0.313 | Hits@   5: 0.358\n",
      "DCG@  10: 0.330 | Hits@  10: 0.412\n",
      "DCG@ 100: 0.369 | Hits@ 100: 0.604\n",
      "DCG@ 500: 0.400 | Hits@ 500: 0.845\n",
      "DCG@1000: 0.416 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#12: imprvd tok v2 (wo forward slash & hyphen) + lowercase & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура-ура, и на предобученном, и на нашем эмбеддингах побиты \\[на наших зависит от прогона] результаты дефолтного простейшего токенизатора. Видимо, на наших данных всё же важнее иметь больших отдельных смысловых единц (части составных слов), т.к., вероятно, мало таких слов, смысл которых не выводится из этих отдельных частей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #13:\n",
    "improved tokenizer v2 + lowercase + removing stop words & stackoverflow embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(words, stop_words=stop_words):\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            yield word\n",
    "\n",
    "def preprocess(data):\n",
    "    return list(remove_stop_words(filter_punctuation(improved_tokenizer_v2(data.lower()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dbbc7db4c24179845124a921c6b931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809e4704a0b74b4d992b2b5a242aed8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.418 | Hits@   1: 0.418\n",
      "DCG@   5: 0.504 | Hits@   5: 0.583\n",
      "DCG@  10: 0.525 | Hits@  10: 0.650\n",
      "DCG@ 100: 0.572 | Hits@ 100: 0.878\n",
      "DCG@ 500: 0.584 | Hits@ 500: 0.974\n",
      "DCG@1000: 0.587 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#13: imprvd tok v2 + lowercase + rmv stp wrds & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(stop_words) = 179\n",
      "stop words in stackoverflow embeddings: ['ours', 'himself', 'will', 'just', 'now', 'o', 'y', 'ain', 'ma', 'mightn', 'needn']\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(stop_words) = }\")\n",
    "print(\"stop words in stackoverflow embeddings:\", [word for word in stop_words if word in so_embeddings])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на предобученных эмбеддингах не изменилось, поскольку они и так почти не содержат стоп-слов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #14:\n",
    "improved tokenizer v2 + lowercase + removing stop words & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_14_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d6535b457e45e68d33588683dd8abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_14_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "273bea42a2d941ba94683bda3ebfa43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.367 | Hits@   1: 0.367\n",
      "DCG@   5: 0.438 | Hits@   5: 0.501\n",
      "DCG@  10: 0.455 | Hits@  10: 0.555\n",
      "DCG@ 100: 0.487 | Hits@ 100: 0.714\n",
      "DCG@ 500: 0.504 | Hits@ 500: 0.845\n",
      "DCG@1000: 0.520 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#14: imprvd tok v2 + lowercase + rmv stp wrds & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на наших эмбеддингах после удаления стоп-слов очень сильно выросло (пред. макс. рез. в #12): больше значащих слов попадает в окно при обучении, а при усреднении векторов слов предложения смысл не смазывается практически ничего не значащими в силу их повсеместности стоп-словами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #15:\n",
    "improved tokenizer v2 + lowercase + removing stop words + PorterStemmer & stackoverflow embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def get_stem(words, stemmer=ps.stem):\n",
    "    for word in words:\n",
    "        yield stemmer(word)\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    return list(get_stem(remove_stop_words(filter_punctuation(improved_tokenizer_v2(data.lower())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af6b3efa482b43998aea06d67655aa80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc1cf03acf54fcbae912387b381aa10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.323 | Hits@   1: 0.323\n",
      "DCG@   5: 0.402 | Hits@   5: 0.474\n",
      "DCG@  10: 0.423 | Hits@  10: 0.541\n",
      "DCG@ 100: 0.475 | Hits@ 100: 0.797\n",
      "DCG@ 500: 0.495 | Hits@ 500: 0.948\n",
      "DCG@1000: 0.500 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#15: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на предобученных эмбеддингах сильно упало, потому что они обучены на целых словах, а не их основах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #16:\n",
    "improved tokenizer v2 + lowercase + removing stop words + PorterStemmer & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_16_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40948eb09a6944ea98753f92ba41a4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_16_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704e3fb89a55418b8d95de4da5433923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.397 | Hits@   1: 0.397\n",
      "DCG@   5: 0.479 | Hits@   5: 0.547\n",
      "DCG@  10: 0.496 | Hits@  10: 0.601\n",
      "DCG@ 100: 0.532 | Hits@ 100: 0.777\n",
      "DCG@ 500: 0.546 | Hits@ 500: 0.880\n",
      "DCG@1000: 0.558 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#16: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество на наших эмбеддингах выросло (по сравнению с #14). Беря основы слов, мы теряем часть информации, а для некоторых слов они вообще могут быть ошибочны, но, видимо, для Word2Vec увеличенная частота слова, а значит и встречаемость в разных контекстах важнее для выделения смысла, по крайней мере, на малых корпусах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #17:\n",
    "improved tokenizer v2 + lowercase + removing stop words + WordNetLemmatizer & stackoverflow embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user0\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def get_lemma(words, lemmatizer=wnl.lemmatize):\n",
    "    for word in words:\n",
    "        yield lemmatizer(word)\n",
    "\n",
    "def preprocess(data):\n",
    "    return list(get_lemma(remove_stop_words(filter_punctuation(improved_tokenizer_v2(data.lower())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2bc068827e4e679e0cb041bdf8e5f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soe_based_ranking = make_validation(so_embeddings, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b378deb3a1451d9be6e1293bd2d84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.409 | Hits@   1: 0.409\n",
      "DCG@   5: 0.495 | Hits@   5: 0.573\n",
      "DCG@  10: 0.515 | Hits@  10: 0.636\n",
      "DCG@ 100: 0.564 | Hits@ 100: 0.872\n",
      "DCG@ 500: 0.576 | Hits@ 500: 0.967\n",
      "DCG@1000: 0.580 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#17: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer & stackoverflow embds\"\n",
    "metrics.update(get_metrics(soe_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество немного снизилось по сравнению с #13. Причина, вероятно, та же, что и в #15: эмбеддинги обучались на ненормализованных словах и потому нормализации данных только мешает. Падение качества не столь сильное, как со стеммингом, поскольку лемматизация, очевидно, не так сильно меняет слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #18:\n",
    "improved tokenizer v2 + lowercase + removing stop words + WordNetLemmatizer & our own embeddings (default window (5), default min_count (5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_18_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c364a4d9e84c54bd84db57dae401f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_18_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad77006f627244ad9724294259c1986e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.390 | Hits@   1: 0.390\n",
      "DCG@   5: 0.456 | Hits@   5: 0.517\n",
      "DCG@  10: 0.474 | Hits@  10: 0.572\n",
      "DCG@ 100: 0.510 | Hits@ 100: 0.750\n",
      "DCG@ 500: 0.523 | Hits@ 500: 0.856\n",
      "DCG@1000: 0.539 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#18: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer & our own embds (win 5, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении собственных эмбеддингов нормализация в виде лемматизации помогает: результат лучше максимального без какой-либо нормализации (эксперимент #14). Но на наших данных нормализация в виде стемминга работает ещё лучше (#16)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #19:\n",
    "improved tokenizer v2 + lowercase + removing stop words + PorterStemmer & our own embeddings (window 2, min_count 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    return list(get_stem(remove_stop_words(filter_punctuation(improved_tokenizer_v2(data.lower())))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_19_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=2, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e863da3fbde4e7b8d5d63b264a1da8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_19_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bfebac1de2433490d3c48b8e35cc5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.397 | Hits@   1: 0.397\n",
      "DCG@   5: 0.479 | Hits@   5: 0.547\n",
      "DCG@  10: 0.496 | Hits@  10: 0.601\n",
      "DCG@ 100: 0.532 | Hits@ 100: 0.777\n",
      "DCG@ 500: 0.546 | Hits@ 500: 0.880\n",
      "DCG@1000: 0.558 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#19: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer & our own embds (win 2, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Уменьшение окна практически не изменило качество, что логично, ведь если слово определяется контекстом и неинформативные (стоп-слова) удалены, то наиболее важные и определяющие слова будут находится рядом с определяемым словом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #20:\n",
    "improved tokenizer v2 + lowercase + removing stop words + PorterStemmer & our own embeddings (window 10, min_count 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_20_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=10, min_count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3cabaefb6e14a8db1198f76657ecfc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_20_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d8242f757847579e869e4c886635cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.397 | Hits@   1: 0.397\n",
      "DCG@   5: 0.479 | Hits@   5: 0.547\n",
      "DCG@  10: 0.496 | Hits@  10: 0.601\n",
      "DCG@ 100: 0.532 | Hits@ 100: 0.777\n",
      "DCG@ 500: 0.546 | Hits@ 500: 0.880\n",
      "DCG@1000: 0.558 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#20: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer & our own embds (win 10, min_cnt 5)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение окна также не изменило качество. Видимо, причина в том, что сами документы у нас очень короткие и даже с окном в 5 (захватывает до 10 слов) общий контекст слова был близок ко всему документу (вопросу)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #21:\n",
    "improved tokenizer v2 + lowercase + removing stop words + PorterStemmer & our own embeddings (window 5, min_count 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_21_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f15f7dc9ab4bc6853b6f153d8b897b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_21_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90af7deae4d4d50bd12fecd105e9fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.392 | Hits@   1: 0.392\n",
      "DCG@   5: 0.473 | Hits@   5: 0.540\n",
      "DCG@  10: 0.492 | Hits@  10: 0.598\n",
      "DCG@ 100: 0.528 | Hits@ 100: 0.773\n",
      "DCG@ 500: 0.541 | Hits@ 500: 0.881\n",
      "DCG@1000: 0.554 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#21: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer & our own embds (win 5, min_cnt 10)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересно, что требование встречаемости слова в хотя бы 0.001% документов вместо 0.0005% немного, но уменьшает качество. Но если учесть, что мы ищем дубликаты вопросов, то это логично: подобные уникальные слова хорошо эти дубликаты выделяют."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment #22:\n",
    "improved tokenizer v2 + lowercase + removing stop words + PorterStemmer & our own embeddings (window 5, min_count 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ex_22_embeddings_trained = train_embeddings(prepare_data(train_data, preprocess), window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2451c3df03e406898a10cf00076b838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "our_embeddings_based_ranking = make_validation(ex_22_embeddings_trained, preprocess, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff7bbd6ba7c4541b8bf6987e5ca0d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.400 | Hits@   1: 0.400\n",
      "DCG@   5: 0.483 | Hits@   5: 0.554\n",
      "DCG@  10: 0.501 | Hits@  10: 0.607\n",
      "DCG@ 100: 0.536 | Hits@ 100: 0.779\n",
      "DCG@ 500: 0.549 | Hits@ 500: 0.882\n",
      "DCG@1000: 0.561 | Hits@1000: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics['experiment'] = \"#22: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer & our own embds (win 5, min_cnt 2)\"\n",
    "metrics.update(get_metrics(our_embeddings_based_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь качество по сравнению с `min_count=5` немного выросло, что подтвержает предыдущее предположение про полезность уникальных слов для данной конкретной задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results comparison\n",
    "Для наглядности даны 4 таблицы: полная таблица экспериментов; она же, но отсортированная по $DCG@1$; по отдельной таблице только с экспериментами на предобученных эмбеддингах и на наших собственных эмбеддингах, обе отсортированы по $DCG@1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete table of experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCG@1</th>\n",
       "      <th>Hits@1</th>\n",
       "      <th>DCG@5</th>\n",
       "      <th>Hits@5</th>\n",
       "      <th>DCG@10</th>\n",
       "      <th>Hits@10</th>\n",
       "      <th>DCG@100</th>\n",
       "      <th>Hits@100</th>\n",
       "      <th>DCG@500</th>\n",
       "      <th>Hits@500</th>\n",
       "      <th>DCG@1000</th>\n",
       "      <th>Hits@1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#1: def tknz &amp; stackoverflow embds</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.341622</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.359693</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.405681</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.431275</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.443923</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#2: def tknz &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.237854</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.250290</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.290426</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.324155</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.346984</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#3: def tknz + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.415</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.502495</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.524731</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.570256</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.583425</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.586335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#4: def tknz + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.314556</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.331864</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.369950</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.400832</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.417272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#5: word_tokenize from nltk + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.399</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.508322</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.554418</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.569235</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.572556</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#6: word_tokenize from nltk + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.229</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.282194</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.297580</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.335722</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.369668</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.386788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#7: improved tokenizer + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.411</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.494767</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.515573</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.561398</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.578823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#8: improved tokenizer + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.304011</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.320441</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.357935</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.389876</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.406951</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#9: word_tokenize from nltk + lowercase + punct filter + split by dot &amp; stackoverflow embds</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.493286</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.513590</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.559540</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.573987</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.577207</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#10: word_tokenize from nltk + lowercase + punct filter + split by dot &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.254</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.303940</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.322647</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.391413</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.408274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#11: imprvd tok v2 (wo forward slash &amp; hyphen) + lowercase &amp; &amp; stackoverflow embds</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.504088</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.525277</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.571488</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.584305</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.587106</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#12: imprvd tok v2 (wo forward slash &amp; hyphen) + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.330406</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.368861</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.399832</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.416154</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#13: imprvd tok v2 + lowercase + rmv stp wrds &amp; stackoverflow embds</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.503815</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.525351</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.571787</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.584471</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.587272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#14: imprvd tok v2 + lowercase + rmv stp wrds &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.437772</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.455323</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.487382</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.504018</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.520267</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#15: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; stackoverflow embds</th>\n",
       "      <td>0.323</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.401690</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.494546</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.500085</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#16: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#17: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer &amp; stackoverflow embds</th>\n",
       "      <td>0.409</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.495106</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.515342</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.563622</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.576218</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.579763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#18: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.509851</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.523453</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.538567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#19: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 2, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#20: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 10, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#21: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 10)</th>\n",
       "      <td>0.392</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.491720</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.527650</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.541417</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#22: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 2)</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.483302</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.500575</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.548788</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.561235</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    DCG@1  Hits@1     DCG@5  \\\n",
       "experiment                                                                    \n",
       "#1: def tknz & stackoverflow embds                  0.285   0.285  0.341622   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.198   0.198  0.237854   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.415   0.415  0.502495   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.261   0.261  0.314556   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.399   0.399  0.486750   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.229   0.229  0.282194   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.411   0.411  0.494767   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.250   0.250  0.304011   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.408   0.408  0.493286   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.254   0.254  0.303940   \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.418   0.418  0.504088   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.261   0.261  0.312750   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.418   0.418  0.503815   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.367   0.367  0.437772   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.323   0.323  0.401690   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.409   0.409  0.495106   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.390   0.390  0.455842   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.392   0.392  0.472841   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.400   0.400  0.483302   \n",
       "\n",
       "                                                    Hits@5    DCG@10  Hits@10  \\\n",
       "experiment                                                                      \n",
       "#1: def tknz & stackoverflow embds                   0.393  0.359693    0.449   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)      0.274  0.250290    0.313   \n",
       "#3: def tknz + lowercase & stackoverflow embds       0.582  0.524731    0.651   \n",
       "#4: def tknz + lowercase & our own embds (win 5...   0.361  0.331864    0.414   \n",
       "#5: word_tokenize from nltk + lowercase & stack...   0.566  0.508322    0.633   \n",
       "#6: word_tokenize from nltk + lowercase & our o...   0.330  0.297580    0.378   \n",
       "#7: improved tokenizer + lowercase & stackoverf...   0.573  0.515573    0.638   \n",
       "#8: improved tokenizer + lowercase & our own em...   0.351  0.320441    0.402   \n",
       "#9: word_tokenize from nltk + lowercase + punct...   0.573  0.513590    0.636   \n",
       "#10: word_tokenize from nltk + lowercase + punc...   0.347  0.322647    0.405   \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...   0.584  0.525277    0.650   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...   0.358  0.330406    0.412   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...   0.583  0.525351    0.650   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...   0.501  0.455323    0.555   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...   0.474  0.423024    0.541   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...   0.573  0.515342    0.636   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...   0.517  0.473777    0.572   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...   0.540  0.491720    0.598   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...   0.554  0.500575    0.607   \n",
       "\n",
       "                                                     DCG@100  Hits@100  \\\n",
       "experiment                                                               \n",
       "#1: def tknz & stackoverflow embds                  0.405681     0.679   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.290426     0.516   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.570256     0.874   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.369950     0.604   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.554418     0.858   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.335722     0.571   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.561398     0.864   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.357935     0.589   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.559540     0.862   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.359731     0.592   \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.571488     0.877   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.368861     0.604   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.571787     0.878   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.487382     0.714   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.474851     0.797   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.563622     0.872   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.509851     0.750   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.527650     0.773   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.535632     0.779   \n",
       "\n",
       "                                                     DCG@500  Hits@500  \\\n",
       "experiment                                                               \n",
       "#1: def tknz & stackoverflow embds                  0.431275     0.879   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.324155     0.783   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.583425     0.973   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.400832     0.844   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.569235     0.969   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.369668     0.838   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.575714     0.971   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.389876     0.838   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.573987     0.970   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.391413     0.840   \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.584305     0.974   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.399832     0.845   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.584471     0.974   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.504018     0.845   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.494546     0.948   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.576218     0.967   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.523453     0.856   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.541417     0.881   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.548788     0.882   \n",
       "\n",
       "                                                    DCG@1000  Hits@1000  \n",
       "experiment                                                               \n",
       "#1: def tknz & stackoverflow embds                  0.443923        1.0  \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.346984        1.0  \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.586335        1.0  \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.417272        1.0  \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.572556        1.0  \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.386788        1.0  \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.578823        1.0  \n",
       "#8: improved tokenizer + lowercase & our own em...  0.406951        1.0  \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.577207        1.0  \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.408274        1.0  \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.587106        1.0  \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.416154        1.0  \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.587272        1.0  \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.520267        1.0  \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.500085        1.0  \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.579763        1.0  \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.538567        1.0  \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.553944        1.0  \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.561235        1.0  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(metrics)\n",
    "df.set_index('experiment', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete table of experiments (sorted by DCG@1 descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCG@1</th>\n",
       "      <th>Hits@1</th>\n",
       "      <th>DCG@5</th>\n",
       "      <th>Hits@5</th>\n",
       "      <th>DCG@10</th>\n",
       "      <th>Hits@10</th>\n",
       "      <th>DCG@100</th>\n",
       "      <th>Hits@100</th>\n",
       "      <th>DCG@500</th>\n",
       "      <th>Hits@500</th>\n",
       "      <th>DCG@1000</th>\n",
       "      <th>Hits@1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#11: imprvd tok v2 (wo forward slash &amp; hyphen) + lowercase &amp; &amp; stackoverflow embds</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.504088</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.525277</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.571488</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.584305</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.587106</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#13: imprvd tok v2 + lowercase + rmv stp wrds &amp; stackoverflow embds</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.503815</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.525351</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.571787</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.584471</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.587272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#3: def tknz + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.415</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.502495</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.524731</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.570256</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.583425</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.586335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#7: improved tokenizer + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.411</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.494767</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.515573</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.561398</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.578823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#17: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer &amp; stackoverflow embds</th>\n",
       "      <td>0.409</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.495106</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.515342</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.563622</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.576218</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.579763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#9: word_tokenize from nltk + lowercase + punct filter + split by dot &amp; stackoverflow embds</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.493286</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.513590</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.559540</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.573987</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.577207</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#22: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 2)</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.483302</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.500575</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.548788</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.561235</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#5: word_tokenize from nltk + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.399</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.508322</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.554418</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.569235</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.572556</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#16: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#20: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 10, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#19: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 2, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#21: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 10)</th>\n",
       "      <td>0.392</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.491720</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.527650</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.541417</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#18: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.509851</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.523453</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.538567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#14: imprvd tok v2 + lowercase + rmv stp wrds &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.437772</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.455323</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.487382</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.504018</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.520267</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#15: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; stackoverflow embds</th>\n",
       "      <td>0.323</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.401690</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.494546</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.500085</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1: def tknz &amp; stackoverflow embds</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.341622</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.359693</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.405681</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.431275</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.443923</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#4: def tknz + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.314556</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.331864</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.369950</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.400832</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.417272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#12: imprvd tok v2 (wo forward slash &amp; hyphen) + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.330406</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.368861</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.399832</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.416154</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#10: word_tokenize from nltk + lowercase + punct filter + split by dot &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.254</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.303940</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.322647</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.391413</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.408274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#8: improved tokenizer + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.304011</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.320441</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.357935</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.389876</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.406951</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#6: word_tokenize from nltk + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.229</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.282194</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.297580</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.335722</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.369668</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.386788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#2: def tknz &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.237854</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.250290</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.290426</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.324155</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.346984</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    DCG@1  Hits@1     DCG@5  \\\n",
       "experiment                                                                    \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.418   0.418  0.504088   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.418   0.418  0.503815   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.415   0.415  0.502495   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.411   0.411  0.494767   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.409   0.409  0.495106   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.408   0.408  0.493286   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.400   0.400  0.483302   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.399   0.399  0.486750   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.392   0.392  0.472841   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.390   0.390  0.455842   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.367   0.367  0.437772   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.323   0.323  0.401690   \n",
       "#1: def tknz & stackoverflow embds                  0.285   0.285  0.341622   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.261   0.261  0.314556   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.261   0.261  0.312750   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.254   0.254  0.303940   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.250   0.250  0.304011   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.229   0.229  0.282194   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.198   0.198  0.237854   \n",
       "\n",
       "                                                    Hits@5    DCG@10  Hits@10  \\\n",
       "experiment                                                                      \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...   0.584  0.525277    0.650   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...   0.583  0.525351    0.650   \n",
       "#3: def tknz + lowercase & stackoverflow embds       0.582  0.524731    0.651   \n",
       "#7: improved tokenizer + lowercase & stackoverf...   0.573  0.515573    0.638   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...   0.573  0.515342    0.636   \n",
       "#9: word_tokenize from nltk + lowercase + punct...   0.573  0.513590    0.636   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...   0.554  0.500575    0.607   \n",
       "#5: word_tokenize from nltk + lowercase & stack...   0.566  0.508322    0.633   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...   0.540  0.491720    0.598   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...   0.517  0.473777    0.572   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...   0.501  0.455323    0.555   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...   0.474  0.423024    0.541   \n",
       "#1: def tknz & stackoverflow embds                   0.393  0.359693    0.449   \n",
       "#4: def tknz + lowercase & our own embds (win 5...   0.361  0.331864    0.414   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...   0.358  0.330406    0.412   \n",
       "#10: word_tokenize from nltk + lowercase + punc...   0.347  0.322647    0.405   \n",
       "#8: improved tokenizer + lowercase & our own em...   0.351  0.320441    0.402   \n",
       "#6: word_tokenize from nltk + lowercase & our o...   0.330  0.297580    0.378   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)      0.274  0.250290    0.313   \n",
       "\n",
       "                                                     DCG@100  Hits@100  \\\n",
       "experiment                                                               \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.571488     0.877   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.571787     0.878   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.570256     0.874   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.561398     0.864   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.563622     0.872   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.559540     0.862   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.535632     0.779   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.554418     0.858   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.527650     0.773   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.509851     0.750   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.487382     0.714   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.474851     0.797   \n",
       "#1: def tknz & stackoverflow embds                  0.405681     0.679   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.369950     0.604   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.368861     0.604   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.359731     0.592   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.357935     0.589   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.335722     0.571   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.290426     0.516   \n",
       "\n",
       "                                                     DCG@500  Hits@500  \\\n",
       "experiment                                                               \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.584305     0.974   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.584471     0.974   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.583425     0.973   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.575714     0.971   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.576218     0.967   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.573987     0.970   \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.548788     0.882   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.569235     0.969   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.541417     0.881   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.523453     0.856   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.504018     0.845   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.494546     0.948   \n",
       "#1: def tknz & stackoverflow embds                  0.431275     0.879   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.400832     0.844   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.399832     0.845   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.391413     0.840   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.389876     0.838   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.369668     0.838   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.324155     0.783   \n",
       "\n",
       "                                                    DCG@1000  Hits@1000  \n",
       "experiment                                                               \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.587106        1.0  \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.587272        1.0  \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.586335        1.0  \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.578823        1.0  \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.579763        1.0  \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.577207        1.0  \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.561235        1.0  \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.572556        1.0  \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.553944        1.0  \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.538567        1.0  \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.520267        1.0  \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.500085        1.0  \n",
       "#1: def tknz & stackoverflow embds                  0.443923        1.0  \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.417272        1.0  \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.416154        1.0  \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.408274        1.0  \n",
       "#8: improved tokenizer + lowercase & our own em...  0.406951        1.0  \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.386788        1.0  \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.346984        1.0  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by='DCG@1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with only embeddings pretrained on stackoverflow (sorted by DCG@1 descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_with_soe = {}\n",
    "metrics_with_our_embds = {}\n",
    "\n",
    "for label, values in metrics.items():\n",
    "    metrics_with_soe[label] = values_with_soe = []\n",
    "    metrics_with_our_embds[label]  = values_with_our_embds = []\n",
    "    for exp_desc, value in zip(metrics['experiment'], values):\n",
    "        if \"stackoverflow\" in exp_desc:\n",
    "            values_with_soe.append(value)\n",
    "        else:\n",
    "            values_with_our_embds.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCG@1</th>\n",
       "      <th>Hits@1</th>\n",
       "      <th>DCG@5</th>\n",
       "      <th>Hits@5</th>\n",
       "      <th>DCG@10</th>\n",
       "      <th>Hits@10</th>\n",
       "      <th>DCG@100</th>\n",
       "      <th>Hits@100</th>\n",
       "      <th>DCG@500</th>\n",
       "      <th>Hits@500</th>\n",
       "      <th>DCG@1000</th>\n",
       "      <th>Hits@1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#11: imprvd tok v2 (wo forward slash &amp; hyphen) + lowercase &amp; &amp; stackoverflow embds</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.504088</td>\n",
       "      <td>0.584</td>\n",
       "      <td>0.525277</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.571488</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.584305</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.587106</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#13: imprvd tok v2 + lowercase + rmv stp wrds &amp; stackoverflow embds</th>\n",
       "      <td>0.418</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.503815</td>\n",
       "      <td>0.583</td>\n",
       "      <td>0.525351</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.571787</td>\n",
       "      <td>0.878</td>\n",
       "      <td>0.584471</td>\n",
       "      <td>0.974</td>\n",
       "      <td>0.587272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#3: def tknz + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.415</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.502495</td>\n",
       "      <td>0.582</td>\n",
       "      <td>0.524731</td>\n",
       "      <td>0.651</td>\n",
       "      <td>0.570256</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.583425</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.586335</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#7: improved tokenizer + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.411</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.494767</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.515573</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0.561398</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.575714</td>\n",
       "      <td>0.971</td>\n",
       "      <td>0.578823</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#17: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer &amp; stackoverflow embds</th>\n",
       "      <td>0.409</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.495106</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.515342</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.563622</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.576218</td>\n",
       "      <td>0.967</td>\n",
       "      <td>0.579763</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#9: word_tokenize from nltk + lowercase + punct filter + split by dot &amp; stackoverflow embds</th>\n",
       "      <td>0.408</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.493286</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.513590</td>\n",
       "      <td>0.636</td>\n",
       "      <td>0.559540</td>\n",
       "      <td>0.862</td>\n",
       "      <td>0.573987</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.577207</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#5: word_tokenize from nltk + lowercase &amp; stackoverflow embds</th>\n",
       "      <td>0.399</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.486750</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.508322</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.554418</td>\n",
       "      <td>0.858</td>\n",
       "      <td>0.569235</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.572556</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#15: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; stackoverflow embds</th>\n",
       "      <td>0.323</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.401690</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.423024</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.474851</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.494546</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.500085</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#1: def tknz &amp; stackoverflow embds</th>\n",
       "      <td>0.285</td>\n",
       "      <td>0.285</td>\n",
       "      <td>0.341622</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.359693</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.405681</td>\n",
       "      <td>0.679</td>\n",
       "      <td>0.431275</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.443923</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    DCG@1  Hits@1     DCG@5  \\\n",
       "experiment                                                                    \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.418   0.418  0.504088   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.418   0.418  0.503815   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.415   0.415  0.502495   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.411   0.411  0.494767   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.409   0.409  0.495106   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.408   0.408  0.493286   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.399   0.399  0.486750   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.323   0.323  0.401690   \n",
       "#1: def tknz & stackoverflow embds                  0.285   0.285  0.341622   \n",
       "\n",
       "                                                    Hits@5    DCG@10  Hits@10  \\\n",
       "experiment                                                                      \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...   0.584  0.525277    0.650   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...   0.583  0.525351    0.650   \n",
       "#3: def tknz + lowercase & stackoverflow embds       0.582  0.524731    0.651   \n",
       "#7: improved tokenizer + lowercase & stackoverf...   0.573  0.515573    0.638   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...   0.573  0.515342    0.636   \n",
       "#9: word_tokenize from nltk + lowercase + punct...   0.573  0.513590    0.636   \n",
       "#5: word_tokenize from nltk + lowercase & stack...   0.566  0.508322    0.633   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...   0.474  0.423024    0.541   \n",
       "#1: def tknz & stackoverflow embds                   0.393  0.359693    0.449   \n",
       "\n",
       "                                                     DCG@100  Hits@100  \\\n",
       "experiment                                                               \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.571488     0.877   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.571787     0.878   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.570256     0.874   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.561398     0.864   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.563622     0.872   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.559540     0.862   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.554418     0.858   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.474851     0.797   \n",
       "#1: def tknz & stackoverflow embds                  0.405681     0.679   \n",
       "\n",
       "                                                     DCG@500  Hits@500  \\\n",
       "experiment                                                               \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.584305     0.974   \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.584471     0.974   \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.583425     0.973   \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.575714     0.971   \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.576218     0.967   \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.573987     0.970   \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.569235     0.969   \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.494546     0.948   \n",
       "#1: def tknz & stackoverflow embds                  0.431275     0.879   \n",
       "\n",
       "                                                    DCG@1000  Hits@1000  \n",
       "experiment                                                               \n",
       "#11: imprvd tok v2 (wo forward slash & hyphen) ...  0.587106        1.0  \n",
       "#13: imprvd tok v2 + lowercase + rmv stp wrds &...  0.587272        1.0  \n",
       "#3: def tknz + lowercase & stackoverflow embds      0.586335        1.0  \n",
       "#7: improved tokenizer + lowercase & stackoverf...  0.578823        1.0  \n",
       "#17: imprvd tok v2 + lowercase + rmv stp wrds +...  0.579763        1.0  \n",
       "#9: word_tokenize from nltk + lowercase + punct...  0.577207        1.0  \n",
       "#5: word_tokenize from nltk + lowercase & stack...  0.572556        1.0  \n",
       "#15: imprvd tok v2 + lowercase + rmv stp wrds +...  0.500085        1.0  \n",
       "#1: def tknz & stackoverflow embds                  0.443923        1.0  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soe_df = pd.DataFrame(metrics_with_soe)\n",
    "soe_df.set_index('experiment', inplace=True)\n",
    "soe_df.sort_values(by='DCG@1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with only our own embeddings (sorted by DCG@1 descending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DCG@1</th>\n",
       "      <th>Hits@1</th>\n",
       "      <th>DCG@5</th>\n",
       "      <th>Hits@5</th>\n",
       "      <th>DCG@10</th>\n",
       "      <th>Hits@10</th>\n",
       "      <th>DCG@100</th>\n",
       "      <th>Hits@100</th>\n",
       "      <th>DCG@500</th>\n",
       "      <th>Hits@500</th>\n",
       "      <th>DCG@1000</th>\n",
       "      <th>Hits@1000</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>#22: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 2)</th>\n",
       "      <td>0.400</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.483302</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.500575</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.535632</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.548788</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.561235</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#16: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#19: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 2, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#20: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 10, min_cnt 5)</th>\n",
       "      <td>0.397</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.478834</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.496430</td>\n",
       "      <td>0.601</td>\n",
       "      <td>0.532493</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.545635</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.558281</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#21: imprvd tok v2 + lowercase + rmv stp wrds + PorterStemmer &amp; our own embds (win 5, min_cnt 10)</th>\n",
       "      <td>0.392</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.472841</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.491720</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.527650</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.541417</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.553944</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#18: imprvd tok v2 + lowercase + rmv stp wrds + WordNetLemmatizer &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.390</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.473777</td>\n",
       "      <td>0.572</td>\n",
       "      <td>0.509851</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.523453</td>\n",
       "      <td>0.856</td>\n",
       "      <td>0.538567</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#14: imprvd tok v2 + lowercase + rmv stp wrds &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.367</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.437772</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.455323</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.487382</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.504018</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.520267</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#4: def tknz + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.314556</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.331864</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.369950</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.400832</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.417272</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#12: imprvd tok v2 (wo forward slash &amp; hyphen) + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.261</td>\n",
       "      <td>0.261</td>\n",
       "      <td>0.312750</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.330406</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.368861</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.399832</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.416154</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#10: word_tokenize from nltk + lowercase + punct filter + split by dot &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.254</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.303940</td>\n",
       "      <td>0.347</td>\n",
       "      <td>0.322647</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.359731</td>\n",
       "      <td>0.592</td>\n",
       "      <td>0.391413</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.408274</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#8: improved tokenizer + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.304011</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.320441</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.357935</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.389876</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.406951</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#6: word_tokenize from nltk + lowercase &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.229</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.282194</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.297580</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.335722</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.369668</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.386788</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#2: def tknz &amp; our own embds (win 5, min_cnt 5)</th>\n",
       "      <td>0.198</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.237854</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.250290</td>\n",
       "      <td>0.313</td>\n",
       "      <td>0.290426</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.324155</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.346984</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    DCG@1  Hits@1     DCG@5  \\\n",
       "experiment                                                                    \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.400   0.400  0.483302   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.397   0.397  0.478834   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.392   0.392  0.472841   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.390   0.390  0.455842   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.367   0.367  0.437772   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.261   0.261  0.314556   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.261   0.261  0.312750   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.254   0.254  0.303940   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.250   0.250  0.304011   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.229   0.229  0.282194   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.198   0.198  0.237854   \n",
       "\n",
       "                                                    Hits@5    DCG@10  Hits@10  \\\n",
       "experiment                                                                      \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...   0.554  0.500575    0.607   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...   0.547  0.496430    0.601   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...   0.540  0.491720    0.598   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...   0.517  0.473777    0.572   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...   0.501  0.455323    0.555   \n",
       "#4: def tknz + lowercase & our own embds (win 5...   0.361  0.331864    0.414   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...   0.358  0.330406    0.412   \n",
       "#10: word_tokenize from nltk + lowercase + punc...   0.347  0.322647    0.405   \n",
       "#8: improved tokenizer + lowercase & our own em...   0.351  0.320441    0.402   \n",
       "#6: word_tokenize from nltk + lowercase & our o...   0.330  0.297580    0.378   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)      0.274  0.250290    0.313   \n",
       "\n",
       "                                                     DCG@100  Hits@100  \\\n",
       "experiment                                                               \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.535632     0.779   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.532493     0.777   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.527650     0.773   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.509851     0.750   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.487382     0.714   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.369950     0.604   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.368861     0.604   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.359731     0.592   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.357935     0.589   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.335722     0.571   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.290426     0.516   \n",
       "\n",
       "                                                     DCG@500  Hits@500  \\\n",
       "experiment                                                               \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.548788     0.882   \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.545635     0.880   \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.541417     0.881   \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.523453     0.856   \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.504018     0.845   \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.400832     0.844   \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.399832     0.845   \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.391413     0.840   \n",
       "#8: improved tokenizer + lowercase & our own em...  0.389876     0.838   \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.369668     0.838   \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.324155     0.783   \n",
       "\n",
       "                                                    DCG@1000  Hits@1000  \n",
       "experiment                                                               \n",
       "#22: imprvd tok v2 + lowercase + rmv stp wrds +...  0.561235        1.0  \n",
       "#16: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#19: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#20: imprvd tok v2 + lowercase + rmv stp wrds +...  0.558281        1.0  \n",
       "#21: imprvd tok v2 + lowercase + rmv stp wrds +...  0.553944        1.0  \n",
       "#18: imprvd tok v2 + lowercase + rmv stp wrds +...  0.538567        1.0  \n",
       "#14: imprvd tok v2 + lowercase + rmv stp wrds &...  0.520267        1.0  \n",
       "#4: def tknz + lowercase & our own embds (win 5...  0.417272        1.0  \n",
       "#12: imprvd tok v2 (wo forward slash & hyphen) ...  0.416154        1.0  \n",
       "#10: word_tokenize from nltk + lowercase + punc...  0.408274        1.0  \n",
       "#8: improved tokenizer + lowercase & our own em...  0.406951        1.0  \n",
       "#6: word_tokenize from nltk + lowercase & our o...  0.386788        1.0  \n",
       "#2: def tknz & our own embds (win 5, min_cnt 5)     0.346984        1.0  "
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_embds_df = pd.DataFrame(metrics_with_our_embds)\n",
    "our_embds_df.set_index('experiment', inplace=True)\n",
    "our_embds_df.sort_values(by='DCG@1', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tY8PxB0j-ThG"
   },
   "source": [
    "### Замечание:\n",
    "Решить эту задачу с помощью обучения полноценной нейронной сети будет вам предложено, как часть задания в одной из домашних работ по теме \"Диалоговые системы\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vymVj8IxO2PO"
   },
   "source": [
    "Напишите свой вывод о полученных результатах.\n",
    "* Какой принцип токенизации даёт качество лучше и почему?\n",
    "* Помогает ли нормализация слов?\n",
    "* Какие эмбеддинги лучше справляются с задачей и почему?\n",
    "* Почему получилось плохое качество решения задачи?\n",
    "* Предложите свой подход к решению задачи.\n",
    "\n",
    "## Вывод:\n",
    "Q: Какой принцип токенизации даёт качество лучше и почему?\n",
    "\n",
    "A: Лучше работает самописный метод, который включает в токены часть символов пунктуации. Такова специфика домена: много терминов, ключевых слов и т.п. включают подобные символы, что немного отличается от обычной лексики.\n",
    "\n",
    "Q: Помогает ли нормализация слов?\n",
    "\n",
    "A: Помогает, если при обучении эмбеддингов её тоже использовали.\n",
    "\n",
    "Q: Какие эмбеддинги лучше справляются с задачей и почему?\n",
    "\n",
    "A: Предобученные. Причина, видимо, в том, что домен тот же, но сам корпус значительно больше.\n",
    "\n",
    "Q: Почему получилось плохое качество решения задачи?\n",
    "\n",
    "A: В принципе плохое качество или плохое качество наших эмбеддингов по сравнению с предобученными? Если второе, то опять же, вероятно, из-за разницы в объеме тренировочных данных, хотя максимальный результат наших (#16 или #22) относительно близко подобрался к максимальному результату на предобученных (#11 или #13). Если первое, то, наверное, причина в качестве тренировочных и/или валидационных данных или же в самом методе: простого среднего векторов слов недостаточно.\n",
    "\n",
    "Q: Предложите свой подход к решению задачи.\n",
    "\n",
    "A:\n",
    "- Взглянуть на данные\n",
    "- Подобрать удачную токенизацию\n",
    "- Отсеять пунктуацию, стоп-слова\n",
    "- Применить ту или иную нормализацию\n",
    "- Обучить эмбеддинги\n",
    "- Если используются предобученные эмбеддинги, то важно предыдущие пункты согласовать с ними\n",
    "- Провести валидацию\n",
    "- Если результат не устраивает, то\n",
    "    - что-либо поменять в предыдущих пунктах (изменить токенизацию, изменить/убрать нормализацию)\n",
    "    - потюнить параметры обучения эмбеддингов\n",
    "    - найти тренировочные данные получше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emODHztAQUQz"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "BIWqBuEa6j0b",
    "uS9FwWNd5a3S",
    "MQk_rolFwT_h",
    "ai48-5vv6j1d",
    "Y60z4t6W6j16",
    "0sUSxk866j1_",
    "J5xWOORI6j2F",
    "tHZqgDTo6j0i",
    "ySQQp0oQt1Ep",
    "LL6_Rjg3InL8"
   ],
   "name": "[homework]simple_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
