В этом задании вам предстоит попрактиковаться в решении задачи машинного перевода. В решении этой задачи вам поможет модель Seq2Seq c использованием механизма внимания, то есть attention. Основная часть работа за вас сделана. Вам остается дописать архитектуру в файл [modules.py](https://drive.google.com/file/d/1HOdTV7jzbPUIxng2ULmUCoOpqU0JqLPG/view?usp=sharing). А именно вам придется дописать классы: Attention, DecoderWithAttention. В [jupyter notebook](https://drive.google.com/file/d/1MG4A_oHU6sl93lbvIEthjjHRab19LO8H/view?usp=sharing) вам остается определить гиперпараметры вашей архитектуры из файла modules.py и запустить процесс обучения сети. При изменении файла modules.py вам достаточно перезапустить ячеку с import и reload этого модуля. Для выполнения работы вам поможет [семинар](https://www.youtube.com/watch?v=d8A1nxoZDDk).

Результатом вашей работы будет готовый jupyter notebook (в формате .ipynb, в этом формате можно скачать файл из Google Colab: file->Download .ipynb) с заполненными пропусками и modules.py также с заполненными пропусками.

Также необходимо прислать файл в формате .pdf (file->print->save as pdf), поскольку его гораздо удобнее открывать и проверять. Пожалуйста, проверяйте pdf файл перед сдачей.

Все файлы сдавать в следующий шаг этого урока.
